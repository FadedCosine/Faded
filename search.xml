<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[NLG的Decoding方法相关论文笔记（3）—— Entmax Transformation]]></title>
    <url>%2F2020%2F06%2F13%2FSparse-Text-Generation%2F</url>
    <content type="text"><![CDATA[Sparse Text GenerationSummary文章提出了entmax的训练和sampling策略，解决文本生成中的退化问题的同时，得益于去训练和预测时都是基于entmax所得的稀疏概率分布，又不会额外引入train-test mismatch。文章中对entmax所得的稀疏概率分布没有细致的证明，而是用于19年ACL的一篇文章的结果。 Research Objective同样是解决文本生成任务当中的退化问题，文中把上一篇介绍的top-k sampling和nucleus sampling的策略称为ad-hoc truncation techniques。但这些策略只在decoding的时候实施，就造成了文本生成问题模型在训练和预测时的mismatch。 Problem Statement这些sampling策略的truncated和renormalized的softmax，只在预测阶段才人为地施加，并没有在training的阶段被学到；而且也很难去对比不同的truncation strategies。文章希望在解决文本退化问题的基础上，解决这些mismatch。 Method(s)Sparse Transformations文章以sparsemax或者说是\alpha-entmax来代替那些ad-hoc的sampling策略。\alpha-entmax的transformation能够内在地生成稀疏的概率分布，而后直接从这个稀疏的概率分布上sample就很自然地避免了文本的退化。具体如下。 19年ACL一篇《Sparse Sequence-to-Sequence Models》定义了\alpha-entmax如下： \alpha -entmax(z_t) := argmax_{\textbf{p }\in \triangle^d}\textbf{ p}^T\textbf{z}_t + H_\alpha(\textbf{p}) \tag{1}其中\textbf{z}_t为timesteps t时模型输出的logits，\triangle^d := \{\textbf{p} \in \mathbb{R}^d \mid \sum_{i=1}^d p_i = 1, \textbf{p} \geq \textbf{0} \}，H_\alpha是Tsallis \alpha-entropy: H_\alpha(\textbf{p}) := \left\{ \begin{aligned} & \frac{1}{\alpha(\alpha - 1)} \sum_j(p_j - p_j^\alpha),& \alpha !=1,\\ & \sum_j p_j \log p_j, &\alpha = 1 \end{aligned} \tag{2} \right.当\alpha = 1时，公式2就成了普通的香农熵的定义；当\alpha=2时，公式2就成了Gini熵；当\alpha \rightarrow \infty时，公式2趋近于0，因此相应地公式1中1-entmax, 2-entmax, \infty-entmax就分别对应softmax，sparsemax和argmax。《Learning Classifiers with Fenchel-YoungLosses: Generalized Entropies, Margins, and Algorithms》中展示了当\alpha > 1时，entmax能够输出稀疏的概率分布，也就是说一些无关的词的概率会真正地为0，而不是一个接近于0的比较小的数。 那如何把entmax应用到训练当中呢？原本的文本生成模型MLE的log-likelihood loss如下： L(\theta) = - \sum\limits_{i=1}^{|S|} \sum\limits_{t=1}^{T_i}\log p_\theta(x_t^i\mid x^i_{]]></content>
      <categories>
        <category>千里怀思月在峰</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>NLP</tag>
        <tag>文本生成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLG的Decoding方法相关论文笔记（2）—— Nucleus Sampling]]></title>
    <url>%2F2020%2F06%2F10%2FThe-Curious-Case-of-Neural-Text-DeGeneration%2F</url>
    <content type="text"><![CDATA[The Curious Case of Neural Text DeGenerationSummary文章介绍了一种基于truncation的Necleus Sampling策略，通过设定概率p，动态地选取每次sampling的tokens集合，并对先前提出的top-k sampling和Samplingh with Temperature的多种sampling策略做出了比较。 Research Objective文本生成中的文本退化的问题。 Problem Statement问题背景与上一篇NLG的论文笔记相同，通过MLE的训练方法所得的NLG模型很容易产生合乎语法但多样性、质量很差的文本，对此GAN方法和其他task-specific的beam search的变种不断被提出。上一篇unlikelihood training的文章是从NLG模型的训练方法上解决文本退化的问题，而本文则在decoding的方法上提出了一个新的解决方案。 文中还提到许多pair-wise的文本生成问题，如机器翻译、data-to-text生成、summarization等此类任务（被称为directed generation）通常以encoder-decoder架构，decoding的方法使用beam search，而其中的output是受到input的限制的，所以output中的重复等文本退化问题都不是什么大问题。而对于想故事生成这些open-ended的文本生成问题而言，其高自由度的特性是文本退化问题的难点。 Method(s)Nucleus Sampling文中提出了一个新的decoding策略：Nucleus Sampling。核心思想是以timestep t的token概率分布来决定sample tokens的集合。设定超参数p，定义top-p的sample token集为最小的V^p \subset V，使得： \sum\limits_{x\in V^p} P(x \mid x_{]]></content>
      <categories>
        <category>千里怀思月在峰</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>NLP</tag>
        <tag>文本生成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLG的Decoding方法相关论文笔记（1）—— Unlikelihood Training]]></title>
    <url>%2F2020%2F06%2F09%2FNeural-Text-Degeneration-With-Unlikelihood-Training%2F</url>
    <content type="text"><![CDATA[Neural Text Degeneration With Unlikelihood TrainingSummary文章介绍了一种新的语言模型训练方法unlikelihood training，在MLE的基础上对于设定的negative candidate中的token使用unlikelihood loss做出限制，用于解决MLE的训练方法中存在的重复和unique token少（模型很少输出富有含义的低频词）这些问题。（我私觉得文中实验的评价指标都是对文章提出的方法有利的 Research Objective文本生成中的文本退化的问题。 Problem Statement最大化log-likelihood的目标函数使得文本生成任务的模型过于单调无趣，高频词被反复地使用，而富有含义的低频词却很少被利用。最近的解决方法是用一些采样和复杂的beam search的变种，但治标不治本。 文本退化的可能原因有三：1、Transformer之类的模型更偏爱重复；2、是人类语言的内在属性，而不是模型的缺陷；3、依赖于固定的语料库，而不能考虑到语言的真实目的。 文章使用unlikehood training，对于真实的target token使用likehood update，对于非真实（其他的）token使用unlikelihood update。 Neural Text Degeneration主要的问题包括，重复和token分布的差距。 重复的问题包括sequence-level的重复和next-token预测的重复。前者体现在greedy decoding的方法n-grams相比真实数据，模型的预测很容易出现重复；后者体现在模型在做t timestep时的next-token prediction时使用前文出现的token的概率要比真实数据时t timestep的token是前文出现的token的概率高。 token分布的差距是指模型生成的文本的unique token的数目要比真实数据的少40%。 The Unlikelihood Training ObjectiveUnlikelihood Training定义一个negative candidates，unlikelihood loss的思想就是降低negative candidates出现的概率，其定义step t的unlikelihood loss为 L_{UL}^t(p_{\theta}(\cdot\mid x_{]]></content>
      <categories>
        <category>千里怀思月在峰</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>NLP</tag>
        <tag>文本生成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[23岁的九局下半，一个新的起点]]></title>
    <url>%2F2020%2F05%2F30%2FThe-introspection-of-the-postgraduate-entrance-examination%2F</url>
    <content type="text"><![CDATA[当我去年九月初得知自己进清华九推复试的时候，我在听热狗的 九局下半。在复试进行到一半，却得知自己没有推免名额的时候我苦笑无奈，更多的是心疼我父母供我奔波的操劳。22岁的我怀着青春遥远的梦，深夜和清本直博的挚友晃荡在清华园里，他问我之后打算怎么办，我说考研，相信一定会有大逆转。 认真开始备考，已是10月初。三个月的复习，从B4000转战B3000， 昨日清欢，到恨觉今日情非旧。我以为在变的，其实只是心未澄澈。水灯节，我和研友一起定下了必上岸的约定，不单单因为吃了两顿黄袍加身馄饨，或是住在台儿庄路出门便是大捷，“我的前途，三分老天爷注定，七分靠我自己。”]]></content>
      <categories>
        <category>千里怀思月在峰</category>
      </categories>
      <tags>
        <tag>反思</tag>
        <tag>考试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记：Neural Machine Reading Comprehension：Methods and Trends]]></title>
    <url>%2F2020%2F05%2F05%2FNeural-Machine-Reading-Comprehension-Methods-and-Trends%2F</url>
    <content type="text"><![CDATA[Neural Machine Reading Comprehension: Methods and TrendsSummary这篇综述十分详尽地介绍了机器阅读理解（MRC）问题的问题背景，以及经典的MRC，并将大体的模型架构分模块，对每个模块都进行了详尽的方法介绍。然而，我觉得这篇文章并没有涉及预训练语言模型在MRC问题当中的应用，实属可惜。 Introduction在2015年CNN &amp; Daily Mail数据集发布之后，机器阅读理解问题得以进行深度学习模型的训练。SQuAD数据集的发布更是加快了基于深度学习的机器阅读理解研究的发展。这一点，从15年到18年底这段时期机器阅读理解的研究论文的指数集增长便可以直观地体现出。 本综述的主要贡献包括： 把各种各样的MRC任务归类为四类：完形填空类、多项选择类、答案抽取类、自由问答类。为每一类任务都介绍了代表的数据集 介绍了基于深度学习的MRC系统的大体模型架构，详细提供每个模块中使用的主要技术的概述和摘要 揭示了MRC研究的一些新趋势，讨论了目前存在的挑战和有待解决的问题，希望能找到未来的研究方向，对其他研究者有所启发。 Tasks &amp; Evaluation MetricsTasks Cloze Test（完型填空类）此类任务类似于常见于英语试题中的完形填空部分。问题是通过删除文章中的一些单词或者命名实体来产生的，要求机器在被删除的空白处填上所缺的单词。任务可以选择提供候选答案，或者不提供候选答案。此类任务的代表数据集有CNN/Daily。 Multiple Choice（多项选择类）此类任务类似于英语试题中的阅读理解部分。要求机器根据所提供的上下文从候选答案中选择问题的正确答案。与完形填空相比，多项选择题的答案并不局限于上下文中的单词或实体。但是，这个任务必须提供候选答案。此类任务的代表数据集有RACE Span Extraction（答案抽取类）尽管完型填空类和多项选择类的任务可以在某些程度上衡量机器在自然语言理解的能力，但依旧有些缺陷。比如说，单纯的文字或实体不足以回答问题，在某些情况下，需要一个完整的句子来回答问题，而且有些实际情况下，提出的问题可能根本就没有相应的答案。那么答案抽取式的任务就可以很好地解决以上的问题。此类任务给定上下文和问题，要求机器从相应的上下文中提取一段文本作为答案，其次根据数据集的不同，有些任务可能存在一些问题从上下文中无法提取答案，则要求机器判断出“No answer”。此类任务的代表数据集有SQuAD、SQuAD2.0。 Free Answering（自由问答类）此类任务是MRC任务中最复杂的一个，给定上下文和问题，要求机器给出自己的解答，而且这个答案可能并不简单地是原文中的某个子串。此类任务的代表数据集有MS MARCO。其中MS MARCO数据集可以说是SQuAD数据集分布之后的又一个里程碑，MS MARCO数据集有四个主要的特征：一是所有的问题都是用户真实提出的；二是每个问题，都有10个由Bing搜索引擎得到的相关文档；三是问题对应的答案都是人为产生的，所以这些答案不仅仅是相关文档中的内容提取，而会包含对相关内容的研究和总结；四是由于是人为给出的答案，所以有可能有些答案是相矛盾的，这也更为机器的理解增添了难度。 不同任务的比较： 文章从construction, understanding, flexibility, evaluation and application五个方面对以上四类MRC任务做出了对比。 完形填空测试任务是最容易构建和评估数据集的。但是，由于填空的形式仅限于原始上下文中的单个单词或名称实体，所以完形填空不能很好地测试机器对自然语言的理解，与实际应用不符。多项选择题为每个问题提供候选答案，因此即使答案不局限于最初的上下文，也可以很容易地对它们进行评估。建立这个任务的数据集并不难，因为语言考试中的多项选择题很容易利用。然而，候选答案导致了合成数据集和实际应用之间的差距。相比之下，答案抽取式任务是一种适中的选择，因为数据集易于构造和评估。此外，它们可以在某种程度上测试机器对文本的理解。所有这些优点都有助于对这些任务进行大量的研究。答案抽取式任务的缺点是其结果受限于原始上下文的子序列，这与现实世界还有一定的距离。自由应答任务在理解性、灵活性和应用维度上表现出优越性，最接近于实际应用。然而，任何事物都有两面性。由于其答案形式的灵活性，在一定程度上很难构建数据集，如何有效地评估这些任务的性能仍然是一个独特的挑战。 Evaluation Metrics（评价指标）针对完形填空类和多项选择类的问题常用accuracy也就是准确率，答案抽取类的问题常用Exact Match，精确匹配度，也就是预测结果和ground truth完全相同的比例。 另外，F1 Score也常被用于答案抽取类的任务当中。此时F1的TP、FP、TN、FN定义如下： tokens in reference tokens not in reference tokens in candidate TP FP tokens not in candidate FN TN precision = TP / (TP + FP)，recall = TP/ (TP + FN)，F1 = 2 PR / (P+ R)。相比于EM，F1的评判标准在预测结果和ground truth有所重叠的时候更为松弛。 而对于自由问答类的问题，常用BLEU（常用于MT等文本生成的任务当中）等评价指标进行评价。 General Architecture目前，经典的基于深度学习的MRC模型主要包括以下几层模块： 嵌入模块：因为机器并不能直接理解自然语言，所有在几乎所有NLP任务当中都需要有一个嵌入模块吧输入的单词转换成定长的向量，来作为这个模型的输入。传统的词表征方法如one-hot或者word2vec以及这些方法和其他语言学特征的结合使用，通常会用来表征一个词当中的语义和句法信息。除此之外，最近由大规模语言预训练模型所得到的上下文词表征在编码上下文信息时也表现出显著的优越性。 特征提取模块：在嵌入模块把文章和问题作词嵌入之后接着输入到特征提取模块。为了更好理解文章和问题，这个模块旨在提取出更多的上下文信息，常用RNN或者CNN来进行提取。 文章-问题交互模块。文章和问题之间的一些相互关系信息在答案预测时常常有举足轻重的作用。这种相互关系的信息可以帮助机器确定文章中的哪一部分的内容是回答问题的关键。此阶段常常使用unidirection或者bidirection的attention机制来完成协同关系信息提取的任务。有时，为了比较完全地提取出文章和问题的相互关系信息，文章和问题之间的交互操作会进行多次，这也正是受到人类进行阅读理解是多次反复阅读来回答问题的经验所启发得出的一种技巧。 答案预测模块：答案预测模块是MRC系统当中的最后一个模块，用以估计文章和问题的输入最终输出对于问题答案的预测。这一模块的具体设计依赖于之前所述的不同的MRC任务。 Methods从上述的四个模块分别详细介绍了这些模块可能使用的方法： EmbeddingsEmbedding的方法有三种，分别是传统的词表征、预训练的上下文表征以及多粒度的联合表征。 传统的词表征包括One-Hot和Distributed Word Representation。 预训练的上下文表征则是由各类语言预训练模型所得到的表征，如CoVE、ELMo、GPT、BERT。 多粒度的联合表征是指word-level的嵌入不能够把丰富的语法、句法以及各类语言学信息完整地嵌入到向量当中，比如part-of-speech以及词缀等，那多粒度的联合表征就是从这些方面入手，把多个粒度的信息嵌入到联合表征当种，如Character Embeddings、Part-of-speech Tags、Name-Entity Tags、Binary Feature of Exact Match (whether a context word is in the question）、Query-Category（问题的种类） Feature Extraction这个特征提取模块通常位于EMbedding层之后，用于分别对文章和问题进行特征的提取。通常就是一个bi-LSTM、CNN或者Transformer。 RNN模型：常用bi-LSTM训练得到word-level或者sentence-level（就是把两层LSTM各自最后一个单元的隐层向量拼接在一起），尽管RNNs的方法对于序列信息的获取十分有效，但是由于其不可并行性导致极其难以训练。 CNNs模型：经典TextCNN得到整个句子的特征提取向量。优点是可以并行训练，且对于局部信息的提出非常精炼有效，但是对于长句子的特征提取能力十分有限。 Transformer模型：Transformer主要依据attention机制，相比于RNN，此模型更容易训练，相比于CNN，此模型能更有效地提取出文本信息种的全局依赖性。QANet就是一个经典的基于Transformer的MRC模型。 Context-Question Interaction通过提取文章和问题中的相互关系信息，MRC模型便能够从中挖掘出进行答案预测的线索。用于提取文章和问题的相互关系信息的模型可以分为one-hop和multi-hop。无论是哪种类型的交互模块，attention机制都起着至关重要的的作用。 MRC的attention机制可以分为unidirectional attention和bidirectional attention Unidirectional AttentionUnidirectional Attention通常是从问题到文章的一个attention操作，用于突出文章中和问题最相关的部分。通常是使用一个sentenc-level的embedding得到问题句子的嵌入向量，而后以此问题句子嵌入向量和文章中单词的词嵌入作attention操作：S_i=f(P_i, Q)。然而这种方法忽略了问题句子中的单词对于答案预测的影响。所以unidirectional attention并不足以充分提取文章和问题之间的相关关系信息。 Bidirectional Attention那为了解决上述方法的缺点便引入了Bidirectional Attention即不仅仅做query-to-context的attention，同时也做context-to-query的attention，得到一个pair-wise的matching矩阵M(i,j)。M(i,j)有文章单词嵌入P_i和问题单词嵌入q_j计算。而后根据M矩阵做一个column-wise的softmax用于表示query-to-context的attention权重，row-wise的softmax表示context-to-query的attention权重。 Multi-Hop Interaction：one-hop的模型文章和问题之间的交互只做一次，不能够系统地理解问题和文章之间的相互信息，而导致不足以解决一些复杂的MRC问题，如从多个句子的文章中做答案的预测。因而便引出了multi-hop的交互。multi-hop的交互模块得益于先前文章和问题的记忆，能够深层次地提取出文章和问题的相互关系，从而得到进行答案预测的线索。 Answer Prediction答案预测模块通常是MRC系统的最后一个模块，这个模块通常是根据不同任务的目标进行定义的，如先前提到的MRC的四类任务。 Word Predictor：常见的做法是用query-aware的context表征去匹配词表或者候选集。 Option Selector：常见的做法是使用attentive的context表征和候选答案的表征进行相似度计算，并选出和context表征相似度最高的候选答案作为最终预测的答案。 Span Extractor：通常是用一个Boundary Model来预测答案的开始和结束的位置。 Answer Generator：通常使用文本的生成模型进行答案文本的生成。 Other Tricks Reinforcement Learning: 大部分的MRC模型只使用极大似然估计来进行训练时的目标决策。然而，这样的目标函数和实际的评价指标并没有直接的联系。这就可能会导致，一些和正确答案有重叠但又不是完全正确的预测结果被模型忽略了，而不能对模型的训练做出一些贡献。而且，当正确答案很长或没有明确的边界时，模型很难提取出完全正确的答案。但是，却不能直接用评价指标如EM、F1等作为目标函数，因为这些评价指标都是不可微分的，不能进行神经网络的训练。因此，Xiong et al.和 Hu et al.便提出使用强化学习的方法，将F1 score作为reward function，把极大似然估计的神经网络的训练和强化学习任务作为多任务学习的问题。除此之外，强化学习的方法也能被用在multi-hop的交互模块决定是否提交交互的进程，如ReasonNets Answer Ranker: 通常的做法是先提取出一个答案的候选集，之后计算这个答案候选集中的候选答案和问题表征的相似度，选择相似度最大的候选答案作为最终答案。 Sentence Selector: 通常，MRC问题的文章是一个非常长的文档，在这一整篇文档中寻找答案是一件非常耗时的事情。然而，找到最相关的句子可能会加速后续的训练过程。Minet al.受此启发，提出了一个句子选择器，来得到真正对回答问题有用的句子最小集。具体来说，这个句子选择器是一个seq2seq的模型，encoder用于计算文章的句子和问题句子的encodings，decoder用来计算文章句子和问题句子的相似度。超过某个阈值的句子将作为最终MRC系统的输入。而且输入句子的数量会根据不同的问题动态选择。 New TrendsSQuAD2.0数据集当中包括了不可回答的问题。对于这些不可回答的问题，就出现了两个新的挑战： Unanswerable Question Detection：模型必须知道他们不知道什么。 Plausible Answer Discrimination：模型必须能从貌似可行的答案中分辨出正确的答案 Open Issues目前MRC模型还有待解决的问题包括以下几点： Lack of world knowledge Robustness of MRC systems Limitation of given context Lack of inference ability]]></content>
      <categories>
        <category>千里怀思月在峰</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>NLP</tag>
        <tag>机器阅读理解</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文笔记：Pre-trained Models for Natural Language Processing：A Survey]]></title>
    <url>%2F2020%2F04%2F30%2FPre-trained-Models-for-Natural-Language-Processing-A-Survey%2F</url>
    <content type="text"><![CDATA[Pre-trained Models for Natural Language Processing: A SurveySummary关于预训练模型的一篇很好的Survey，总结了从word2vec时代静态的文本表征到目前后BERT时代，PTMs提取出的动态文本表征。PTMs的不同主要体现在模型所用的上下文encoder不同、预训练模型的任务不同、以及PTMs所针对的下游任务等。 Research Objective关于语言预训练模型的一个survey BackgroundNeural Contextual Encoders神经网络的上下文编码器： Covolutional model：通过卷积操作提取局部的上下文信息。优点是非常容易训练，并且能够获取局部信息 Sequential models：序列模型如RNNs，bi-RNN用于提取上下文信息，但是序列模型的表现经常受到文本中的长距离依赖因素的影响。 Graph-based model：基于图的模型通常把文本看作是节点，使用一些语言学的结构，如语义结构、句法结构来训练上下文表征。 注意相应的参考文献：《Exploitingsemantics in neural machine translation with graphconvolutional networks》值得一读。 缺点是得到这样的一个语言学结构特别困难，而且也非常依赖一些其他的解析器，如句法依存分析的解析器。实际上的做法是用self-attention，比如Transformer，让model自己去学习这些潜在的语言学结构信息，也就是并不用一些其他显式的做法。 虽然Transformer通过对每个词之间的dependency建模，解决了卷积方法和序列方法的长距离依赖问题，但这又导致Transformer需要很大的训练集，如果训练集小了又容易过拟合。 Overview of PTMs不同的PTM的区别主要在contextual encoders、pre-training tasks, and purposes，之前的Neural Contextual Encoder一节已经简单介绍过了contextual encoders。 Pre-training Tasks在CV领域的预训练模型通常是在大规模的监督学习数据集中训练的，如ImageNet。而在NLP领域，除了在machine translation有大规模的有监督数据集，其他nlp领域有监督的数据集匮乏。而其实也有基于MT领域的有监督数据集训练的预训练语言模型CoVe。 Language Model p(x_t|x_{0:t-1}) = g_{LM}(f_{enc}x_{0:t-1})表示$x{0:t-1}$用neural encoder$f{enc}(·)$做了上下文嵌入，经过prediction层$g_{LM}(·)$得到下一个单词$x_t$的条件概率。 Masked Language Modeling（MLM) BERT的MLM的做法是：随机mask语料中15%的token，然后预测masked token，那么masked token 位置输出的final hidden vectors喂给softmax网络即可得到masked token的预测结果。 这样操作存在一个问题，fine-tuning的时候没有[MASK] token，因此存在pre-training和fine-tuning之间的mismatch，为了解决这个问题，采用了下面的策略： 80%的时间中：将选中的词用[MASK]token来代替，例如 my dog is hairy → my dog is [MASK] 10%的时间中：将选中的词用任意的词来进行代替，例如 my dog is hairy → my dog is apple 10%的时间中：选中的词不发生变化，例如 my dog is hairy → my dog is hairy 这样存在另一个问题在于在训练过程中只有15%的token被预测，正常的语言模型实际上是预测每个token的，因此Masked LM相比正常LM会收敛地慢一些，后面的实验也的确证实了这一点。 MLM也存在一些变种，如Seq2Seq MLM，主要的预训练模型包括MASS和T5，显而易见，这个Seq2Seq的MLM所训练出来的预训练模型在Seq2Seq的下游任务肯定会有提升，比如说QA，summarization和MT。Enhanced Masked Language Modeling (E-MLM)：RoBERTa把静态的Masking改成了动态的，即一开始把预训练的数据复制10份，每一份都随机选择15%的Tokens进行Masking，也就是说，同样的一句话有10种不同的mask方式。然后每份数据都训练N/10个epoch。这就相当于在这N个epoch的训练中，每个序列的被mask的tokens是会变化的。ERNIE的改进是不是mask一个单词，而是mask一些短语或者说命名实体（我觉得这样的预训练方式可能会对answer-span类型的QA有所改进） Permuted Language Modeling (PLM)因为MLM在训练时的一些特殊token如[MASK]在下游任务用于预测的实际文本中是不存在的，这就导致在train和fine-turning之间可能会出现差距。那PLM的做法就是，给定一个句子，从这个句子中随机采样一个排列，接着将排列序列中的一些 token 选定为目标，同时训练模型以根据其余 token 和目标的正常位置（natural position）来预测这些目标。 Denoising Autoencoder (DAE)去噪自编码器（denoising autoencoder，DAE）接受部分损坏的输入，并以重建这些未失真的原始输入为目标。在自然语言处理的下游任务中，常用Transformer来重建文本。在构建训练文本时有一些不同的制造噪声的方式。 Contrastive Learning (CTL)一些文本对在语义上比随机取样的文本更为接近。CTL 背后的原理是「在对比中学习」。相较于语言建模，CTL 的计算复杂度更低，因而在预训练中是理想的替代训练标准。CTL包括 Replaced Token Detection (RTD) 与Noise-Contrastive Estimation (NCE就相当于word2vec中的负采样) 相同，但RTD会根据上下文语境来预测当前的token是否是被替换过的token。ELECTRA就是这样一个牛批的预训练模型。ELECTRA在以RTD为训练目标的同时还用了生成对抗模型的思想。ELECTRA和BERT最大的不同应该是在于两个方面：masked(replaced) tokens的选择和training objective。 第一，masked token的选择在BERT中是随机的，这意味着什么呢？比如句子“我想吃苹果”，BERT可以mask为“我想吃苹[MASK]”，这样一来实际上去学它就很简单，如果mask为“我[MASK]吃苹果”，那么去学这个“想”就相对困难了。换句话说，BERT的mask可能会有很多简单的token，去学这些token就算是简单的bilstm都可以做的。这样一来，自然而然的一个想法是，我们不随机mask，去专门选那些对模型来说学习困难的token。怎么做呢？这就是ELECTRA非常牛逼的地方了，ELECTRA有一个生成器和鉴别器，生成器将用来把输入文本做部分词的替换，而鉴别器判断输入句子中的每个词是否被替换。也就是说，这里的生成器的作用是为自动选择tokens来替换成其他token，并且经过训练，生成器会越来越选择 一些比较难的token替换，希望骗过鉴别器。在鉴别器方面，用一个二分类去判断每个token是否已经被换过了。这就把原来BERT的一个DAE（或者LM）任务转换为了一个分类任务（或者序列标注）。这有两个好处：（1）每个token都能contribute to some extent，即每个token对模型都有所贡献；（2）极大程度减少了计算复杂度，原来BERT的复杂度是O(d*V)，d是输入文本长度，V是词表大小，那ELECTRA的复杂度就是O(d*2)。 Next Sentence Prediction (NSP) 训练模型以区分两个输入句子是否为训练语料库中的连续片段。正例是同一个文档的两个连续片段，负例是语料库中的随机两个文档。实际上这个训练目标并不是特别好，因为很容易根据两个句子的主题判断是否是连续的片段，从而可能会错误地引导模型去做主题预测的工作。 Sentence Order Prediction (SOP) SOP 使用同一文档中的两个连续片段作为正样本，而相同的两个连续片段互换顺序作为负样本。这样就不存在主题不同的问题了。 Model Analysis Linguistic Knowledege (语言知识) BERT在句法任务中的表现很好，但却在语义和细粒度的句法任务上表现不佳。 World Knowledge BERT能够提取文本中的World Knowledge，如一些关系信息和常识。 Extensions of PTMs Knowledge-Enriched PTMs 加入了一些额外的信息增强了预训练模型。如改进MLM，在每个词上整合这个词的情感词性所提出的SentiLR；K-BERT显式地把从知识图谱中提取出的关系实体嵌入到BERT当中，使得以一个树形的数据作为BERT的输入；K-Adapter通过对不同的预训练任务训练不同的adapter来对不多种额外的信息做一个合适的整合，等等。 Multi-Modal PTMs暂且不看 Model Compression Adapting PTMs to Downstream Tasks尽管预训练模型能够捕获文本的general knowlegde，但怎么样高效的适用于下游任务已让是一个关键的问题。 把PTM适用于下游任务是一个顺序的迁移学习任务，即顺序地学习任务，并且目标任务是带有标签的。 那么如何做预训练模型到下游任务的迁移呢？ 选择一个合适的预训练任务、模型架构和下游任务训练语料库。 选择合适的网络层。可以选择只用BERT的第一层Embedding层，也可以直接在BERT的输出上加一层Top Layer，也可以把每一层的输入做一个加权，作为Encoder的输出。 fine-tuning的技巧：mult-task learning、使用额外的适配模块进行fine-tuning、gradual unfreezing]]></content>
      <categories>
        <category>千里怀思月在峰</category>
      </categories>
      <tags>
        <tag>论文笔记</tag>
        <tag>NLP</tag>
        <tag>预训练模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2020拼多多算法实习生笔试、面试记录]]></title>
    <url>%2F2020%2F04%2F16%2FThe-Entrance-Test-and-Interview-of-Pinduoduo%2F</url>
    <content type="text"><![CDATA[笔试拼多多笔试两小时四道题，做出来三道，最后一道题未果，后寻得答案。 N个方块涂有颜色，玩家可以从所有的方块中任意移除最多k个方块，使得在剩余的方块中，连续相同的颜色的方块长度最长。问通过移动，可以得到的相同颜色的方块最长多长。 起初以为是用dp来做，不过确实能做：12345678910//dp[i][j]表示以[i]结尾，移动j个方块所能得到的最长长度；//状态转移方程为：if(a[i] == a[i-1])&#123; dp[i][j] = max(dp[i][j], dp[i-1][j] + 1);&#125;else&#123; dp[i][j] = max(dp[i][j], dp[i-1][j-1]);&#125; 不过这种做法，需要的时间复杂度为O(nk)，超时了，显然不能满分。 实际上，正确的解法为使用滑动窗口的技巧，对于初始情况下的每一块相同颜色的色块，以该色块颜色为当前的目标颜色，维护最大移除长度为k的滑动窗口（即最多跳过k个不相同的颜色方块），记录当前窗口内，通过移除可得的最大相同长度，在依此更新全局的最大长度。不过，我寻思这样做时间复杂度也是O(nk)，只不过可能常数要小一些而已。代码如下：12345678910111213141516171819202122232425262728293031323334353637#include&lt;iostream&gt;#include&lt;algorithm&gt;using namespace std;int cicada[100010];int n, k;int main()&#123; cin&gt;&gt;n&gt;&gt;k; for(int i = 0 ; i &lt; n; i++) cin&gt;&gt;cicada[i]; int i = 0, ans = 0; while(i &lt; n) &#123; int len = 1; while(i + 1 &lt; n &amp;&amp; cicada[i] == cicada[i+1]) //维护初始色块 &#123; i++, len++; &#125; int t = 1, count = 0; while(i + t &lt; n - 1 &amp;&amp; count &lt; k) &#123; if(cicada[i] == cicada[i + t]) len++; else count++; t++; &#125; while(i + t &lt; n &amp;&amp; cicada[i] == cicada[i + t]) &#123; t++, len++; &#125; ans = max(anx, len); i++; &#125; cout &lt;&lt; ans &lt;&lt; endl; return 0;&#125; 一面面试官是一个和蔼的叔叔（真滴好和蔼呀，整个面试的过程都轻松了很多呢）太和蔼了，好感度+10，终于不会被庄小姐杀死而潜伏失败了。令我吃惊的是，面试官先向我做了一个自我介绍，他是拼多多图像组的组长，工作内容是拼多多内部或者产品的一些人脸识别、图像搜索等内容。之后，大抵寻常的过程，我做了自我介绍，而他针对我的项目问了很多问题，以下作为记录： 问：你的这篇论文工作是在哪里一些改进呢？你说是系统的响应时间，怎么看着你们使用社交网络的维度是在推荐系统的效果上做出了一些改进呢？ 答：是的，考虑社交网络关系对于用户查询的影响的确能够在推荐系统的推荐效果上给用户以更好的对象推荐，那我所说的在响应时间上的改进是得益于我们使用用户的社交网络影响进行了一些剪枝算法的设计。首先我们使用图嵌入算法得到用户特征向量之后，使用用户之间的相似度对于用户朋友对于每个索引节点的访问次数进行加权求和之后归一化，得到用户受其社交关系的影响因子，并设定一个阈值，我们认为低于这个阈值的索引节点是在社交与当前用户不相关的节点，就剪枝掉，从而节省了很多对于不相关节点的访问时间。 问：你的鸟类图像分类这个项目中为什么要做边缘检测、去背景，难道去背景之后效果会更好嘛？ 答：我们拿到的图片中，有些鸟类主体只占图片的非常小的一部分，我们觉得去除多余的背景，可以去除一些冗余的信息，可能会提高模型的效果。 问：那你去了背景之后模型效果提高了多少。 答：我没做对比实验 之后，尬住。。 问：你毕设项目中的这个验证器是什么？能具体介绍一下嘛？ 答： 问：介绍一下CycleGAN。 答：我大概说了一下，这里记录一下自己说错的一点。实际上，Cycle Consistencey Loss包括的是x -&gt; G(x) -&gt; F(G(x))中F(G(x))相对于x的loss（这个叫forward cycle consistency），还包括y -&gt; F(y) -&gt; G(F(y))中G(F(y))相对于y的loss（这个叫backward cycle consistency）。两者之和才是cycle consistency loss。 问：你的deblur项目中改进了CycleGAN模型的什么网络结构？ 答：我们改进了CycleGAN的生成器网络，用了类似于Amulet的方法，使用多层次的特征提取，把低层次的卷积特征通过上采样的一些反卷积的操作，和后一层的feature map做concat操作。这样做就是希望一些低分辨率的粗糙去模糊图片引导高分辨率的去模糊图片的生成。 （我自己）问：什么是反卷积？ （知乎）答：在应用在计算机视觉的深度学习领域，由于输入图像通过卷积神经网络(CNN)提取特征后，输出的尺寸往往会变小，而有时我们需要将图像恢复到原来的尺寸以便进行进一步的计算，这个采用扩大图像尺寸，实现图像由小分辨率到大分辨率的映射的操作，叫做上采样。而反卷积是一个常用的上采样方法，那反卷积是一种特殊的正向卷积，先按照一定的比例通过补0来扩大输入图像的尺寸，再进行正向卷积（也就是根据要上采样到的图片大小，进行补0，在对补0之后的矩阵做正向卷积）所以反卷积只能恢复尺寸，而不能恢复数值。 问：SSIM loss是什么？ 答：SSIM loss是基于样本x和y之间三个方面的比较，分别是亮度、对比度和结构，SSIM loss改进了MSE不能衡量图像结构相似性的缺陷（SSIM越大，两个图像之间的差距越小）。 问：介绍一下3D表面重建的Matching Cube算法。 答：首先我们这个点云空间被分为k k k个小方块，也就是所谓的体素，存在模型点的体素，我们称为实体素，不存在的成为虚体素。定义体元是由8个相邻的体素所构成的正方体。而我们要做的3D表面重建就是在做3D模型表面的三角形面片的重建。那位于3D模型表面的体元的8个体素点都可能是实体素点或者虚体素点，那只有一共有2的8次方种情况，也就是256种情况。Matching Cube的思想就是利用这256种可枚举的情况来进行体元内3D模型表面的抽取。那这么抽取呢，举个例子，如果在边界体元中，A体素是实体素，B体素是虚体素，那这个表面就一定经过A和B的中点位置。也就是根据这样的表面划分方法，通过关注边界的体元，我们就能进行3D模型的表面重建。 问：我看你都是在做一些深度学习的内容，你对传统的机器学习了解吗。介绍一下GBDT。 问：介绍一下决策树的种类。（我并不是很懂这个问题，于是说了决策树的生成过程，这里试着回答这个问题。）]]></content>
      <categories>
        <category>世若秋水一般凉</category>
      </categories>
      <tags>
        <tag>笔试</tag>
        <tag>面经</tag>
        <tag>反思</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[广告文本多分类——2020腾讯游戏安全技术决赛反思]]></title>
    <url>%2F2020%2F04%2F14%2FThe-introspection-of-Tencent-game-security-technology-competition%2F</url>
    <content type="text"><![CDATA[四月初的天气时常阴晴未定。 由于好兄弟去年参加了这个比赛的缘故（见他的博客），我今年也尝试报名参赛，并也进入决赛，不过可惜只拿到优秀奖。我自己甚是失望，故属文于此，记录下整个答题过程和反思。 1. 数据分析赛事方提供的游戏部分公开言语数据（中文），每条记录包括了言语内容本身和对应的细分类标签（正常和4种广告细分类，共5个分类）。首先我对数据进行初步分析，得到数据的数量分布以及对应的标签含义如下表： 数据标签 0 1 2 3 4 数量/条 73034 25466 500 1000 2000 标签含义 普通非广告文本 出资源广告 退款广告 社交广告 代练广告 ​ 由此可见，各类标签的数据十分不均衡，因此我做了以下两种数据增强操作。 1) 分词数据增强：我首先设定均衡的数据分布比例为4:1:1:1:1，而后我使用jieba分词将文本进行分词操作。分词之后，对于文本的分词进行两种操作，一是随机地进行删除，二是 shuffle, 即打乱词序，得到通过分词进行数据增强之后的训练集； 2) 反向翻译数据增强：反向翻译的主要思想是先将机器翻译成另一种语言，再从另一种语言的文本翻译回原先语言，从而进行数据增强。我按照设定的数据分布比例，使用腾讯AI开放平台提供的文本翻译（翻译君）API进行反向翻译数据增强。 经过数据增强，各标签数据的比例大约在4:1:1:1:1，总的数据量到达了293490条。 2. 模型建立我实际上实现了两个模型，BERT与TextCNN。 因为Pytorch和Transformers的框架下，就已经有了文本分类的API，用BERT的中文分词器处理完数据之后就可以很简单的直接训练，具体可以参考我滴好兄弟的另一篇博客。 而TextCNN则十分常规的embedding+convd+maxpooling+output的网络结构，实现起来也轻而易举，分词则是用jieba这个分词库。 以上就是就是我模型的建立，由于本文重点在反思部分，故此处便不再叨唠。 3. 测试效果BERT的多分类模型，在我9:1划分的测试集上的结果如下： Accuracy Precision Recall F1 97.5926% 97.76% 97.80% 97.71% 看着这么高的表现只不过是镜花水月。实际上我得知自己只有优秀奖之后向赛事方寻求了测试集的部分数据，其数据的数量分布如下： 数据标签 0 1 2 3 4 数量/条 12110 2447 287 620 1269 可以发现，测试集的数据分布其实和训练集是十分接近的，而我训练的BERT模型在测试集上的测试效果则不堪入目： Accuracy Precision Recall F1 87.0805% 27.85% 17.23% 21.78% 而TextCNN的模型在测试集上却有不错的效果： Accuracy Precision Recall F1 86.85% 88.87% 60.31% 79.46% 认真debug一晚上之后，发现是在BERT模型中做预测时，调用forward的时候少传了一个参数，预测部分的代码如下： 12345678910111213141516171819..........for step, batch in enumerate(test_dataloader): if step % 40 == 0 and not step == 0: print(' Batch &#123;:&gt;5,&#125; of &#123;:&gt;5,&#125;.'.format(step, len(test_dataloader))) batch = tuple(t.to(device) for t in batch) # b_input_ids, b_input_mask = batch b_input_ids, b_input_mask, b_labels = batch with torch.no_grad(): outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask #在提交答卷时少传了这个attention_mask ) logits = outputs[0] logits = logits.detach().cpu().numpy() label_ids = b_labels.to('cpu').numpy()............. attention_mask的作用是用于指示输入的文本，如果是PAD符号则是0，否则就是1。无论在训练还是预测，这都是一个十分重要的参数，我竟然漏了这个参数，实属nt。还有一个很重要的原因是，由于本地GPU不够跑BERT，而我又暂时没有服务器，只能在colab上跑程序，当时做预测时，只用了非常小（10行）的一个测试集，发现都正确之后便没有再检查了。出现如此低级错误，实在是不能接受。 不过现在的懊恼都已经弃我去者，昨日之日不可留了。希望以此为戒，乞求拜托今日之日的烦忧。 debug之后，BERT模型正确的测试代码的测试结果如下： Accuracy Precision Recall F1 87.08% 88.20% 61.57% 79.60% 相比于TextCNN只有一点点的提升。考虑到测试文本中存在很多的非中文噪声，如：（该例子为我手动杜撰的） 14t把恩dsaiuhf223daf2元十万钻 实际上这个广告文本的内容就是 12元十万钻 所以我清洗了文本中的非中文字符。注意，可能存在其他正常的文本中本就出现非中文字符，所以这种清洗方式可能并不是最合适的。清洗之后BERT模型的表现略有上升： Accuracy Precision Recall F1 87.33% 88.61% 62.35% 80.17% 以上，这次大赛的工作就告一段落了。之后，我向主办方了解到广告文本多分类第一名的F1达到了89.3%，确实存在比较大的差距。总结来说我觉得自己的工作可以改进的部分如下： 对于文本多分类问题，还只是用了最基础的BERT预训练模型加上一层decoder，可能忽略了其他一些重要的信息，decoder可能过于简单，也许可以参考最近的论文做更多设计。 可能数据的预处理部分存在问题，数据增强部分设定的数据分布比例可能必不合理，相反强行拉成4:1:1:1:1可能造成模型从这个数据分布获取了一些其他并不准确的特征信息。另外，数据清洗部分的做法也有待考虑。 我以后一定要换一台本地可以跑BERT的代码，或者自己有一张卡，不愿上colab薅资本主义恶臭的的羊毛（其实是因为colab老是断开连接，GPU用久了还给我限额）。 面试在听 Jony J 的 My Man 时收到了腾讯面试官的来电，便开始了一次忽如而至的面试，听声音面试官是个和蔼的靓仔。 面试的流程大抵寻常，自我介绍、项目介绍、针对项目进行提问。 我介绍完自己用ALBERT做的机器阅读理解的毕业设计之后，他问了我一些问题： 问：ALBERT相较于BERT的改进。 答：极大程度地减少了参数量； 问：Multi-Head Attention的作用。 答：让模型去关注不同方面的信息，比如说一些底层的Head去关注一次低层的识别，如关注位置信息、关注语法信息、关注罕见词，而高层的Head去对整个句子做一些高层次的理解； 问：前馈神经网络的作用。 答：其实就是全连接层的作用，Multi-Head Attention/卷积对数据进行处理后得到了数据的局部特征，而前馈神经网络/全连接层的作用就是把之前的局部特征通过权值矩阵进行整合，方便进行后续下游任务的处理； 问：你为什么认为BERT这些预训练模型不能提取文本信息中的句法和语义信息？ 答：我是从BERT模型的训练方式MLM和Next Sentence Prediction，说明其只能对上下文信息做比较好的提取，而没有显性考虑文本中的语义和句法信息； 问：怎么理解语义？你对语义是怎么处理的？ 问：你们又是怎么处理句法的呢？ 问：你说用了斯坦福的句法依存分析的API，介绍一下这个句法依存分析的原理： 问：句法和语义信息中的位置信息你们是怎么处理的呢？ 接着又让我介绍了一下我的论文工作，针对我的论文问的问题我大多都回答得不错，其中有些印象深刻的问题： 问：对于推荐系统中的冷启动问题，你们是怎么处理的？ 答：（其实没有处理）emmm，确实存在冷启动问题，那我们使用的解决冷启动问题的方法是设定了一个地理位置信息对象的种类熵，也就是把熵的概念应用在店铺的种类分布之下，把这个种类熵作为基于索引的搜索过程中的一种评价指标，熵越大表示当前索引节点下的店铺种类越多，而这样的做法恰恰也是倾向于给用户提供多样性的选择，也就是推荐系统当中一种常用的解决冷启动问题的方法。 问：相对于Baseline，你们的模型取得了怎么样的提升？ 答：从响应时间上对比，100条真实的搜索，我们的平均响应时间为15秒，也就是平均每条0.15秒。另外我们实现了传统的Spatial Keywords Query中的IR树模型作为baseline，而这个模型的响应时间，100条真实的搜索，平均响应时间为30秒，也就是平均每条0.3秒。也就是说我们的模型快乐以北，那作为trade-off，由于我们的模型引入了新的维度的特征，在索引的大小上有了一定度的增大。 最后让我介绍了这次大赛的工作，我主要说了自己做的数据增强工作（说出来其实很水），接着面试官小哥问了一些问题： 问：如何对文本是如何做分词的呢？ 答：我说用BERT原来的中文tokenization，但实际上原来的tokenization并不是传统意义上的分词，而是把文本分成一个个单字，而BERT就是基于单字的表示，也就相当于BERT模型并没有做中文的分词；另一个textcnn的模型倒是用了分词，用jieba分词之后，而后使用随机词向量输入到textcnn模型中。 以上便是对这次面试的大多印象，这个实习岗位是在腾讯深圳总部，工作内容是关于非法信息识别（祖安人亲妈）。希望能顺利拿到offer。]]></content>
      <categories>
        <category>人如春色三分淡</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>反思</tag>
        <tag>预训练模型</tag>
        <tag>竞赛</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Odyssey of DP —— NOI题集拾遗]]></title>
    <url>%2F2020%2F02%2F12%2FThe-Odyssey-of-DP-2%2F</url>
    <content type="text"><![CDATA[NOI 162:Post Office 在一条直线坐标轴上有V个村庄，P个邮局，邮局建在村庄上，求一种建法，让V个村庄到最近邮局的距离之和最小。输入：第一行包括两个整数， 1 &lt;= V &lt;= 300，1 &lt;= P &lt;= 30；第二行输入有序的村庄的位置a[i]输出：输出最小的V个村庄到最近邮局的距离之和输入10 51 2 3 6 7 9 11 22 44 50输出8 首先需要考虑到，如果对$[0, \cdots , i]$的村庄只建一座邮局，那一定是把邮局建在中央的村庄[i/2]中使得到邮局的距离和最小。以下给出证明：设对于$[0, \cdots , i]$的村庄，将邮局建在$[i/2]$中得到的距离和为S，下标为i的村庄的位置为a[i]。假设只建一座邮局时，邮局建筑的最优位置为$[j]（j \ne i）$，则邮局建在[j]的距离和为$S+ abs(a[i/2] - a[j]) \times (i - i/2 - j)$，因为 $S+ abs(a[i/2] - a[j]) \times (i - i/2 - j) &gt; S$，与 [j]为最优位置矛盾，所以 j = i / 2，即得证。我们可以设置$dp[i][j] := [0, \cdots , i]$村庄中建j座村庄的距离和最小值。由之前证得的定理很容易得到状态转移方程：(min_distance(i,j)表示在$[i, \cdots, j]$建一座邮局的距离和最小值) dp[i][j] = \min \{ dp[m][j-1] + min\_distance(m+1, i)\} | 0 \le m < i 即对$0 \le m &lt; i ，[0, \cdots, m]$的村庄建j-1个邮局，$[m+1, \cdots, i]$的村庄建一个邮局，遍历m，得到距离和最小值。代码如下，时间复杂度为$O(n^3)$：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#pragma once#include&lt;iostream&gt;#include&lt;cstdio&gt;#include&lt;math.h&gt;#include&lt;algorithm&gt;using namespace std;int p, v;int villages[310];int dp[310][31];// dp[i][j] 表示[0, ... , i]中j个post office的costconst int INF = 1000000000;int min_distance(int i, int j)&#123; int mid = villages[(i + j) / 2]; int ans = 0; for (int k = i; k &lt;= j; k++) &#123; ans += abs(villages[k] - mid); &#125; return ans;&#125;void NOI_2_6_162()&#123; scanf("%d%d", &amp;v, &amp;p); for (int i = 0; i &lt; 310; i++) &#123; for (int k = 0; k &lt; 31; k++) &#123; dp[i][k] = INF; &#125; &#125; for (int i = 0; i &lt; v; i++) &#123; scanf("%d", &amp;villages[i]); &#125; for (int i = 0; i &lt; v; i++) &#123; dp[i][1] = min_distance(0, i); &#125; for (int k = 2; k &lt;= p; k++) &#123; for (int i = 0; i &lt; v; i++) &#123; for (int m = 0; m &lt; i; m++) &#123; dp[i][k] = min(dp[i][k], dp[m][k - 1] + min_distance(m + 1, i)); &#125; &#125; &#125; printf("%d\n", dp[v-1][p]);&#125; NOI 7627:鸡蛋的硬度 有n层楼，m个鸡蛋，如果鸡蛋从第a层摔下来没摔破，但是从a+1层摔下来时摔破了，那么就说这个鸡蛋的硬度是a。这些鸡蛋硬度相同，在求鸡蛋的硬度下问使用最优策略在最坏情况下所需要的扔鸡蛋次数。输入100 1100 2输出10014]]></content>
      <categories>
        <category>千里怀思月在峰</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>DP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Odyssey of DP —— 最大子矩阵]]></title>
    <url>%2F2020%2F01%2F16%2FThe-Odyssey-of-DP-1%2F</url>
    <content type="text"><![CDATA[1. 最大连续子序列和 给定一个数字序列 $A_{1}, A_{2}, \cdots, A_{n}$，求i，j（1$\le i \le j \le n$），使得$A_{i} + \cdots + A_{j} $最大，输出这个最大和。 样例： -2 11 -4 13 -5 -2 显然 11+(-4)+13=20为和最大的选取情况，因此最大和为20 使用动态规划，复杂度为O(n)的做法：设置dp[i]表示以A[i]作为末尾的连续序列的最大和，如此一来，要求的最大和其实就算dp[0]，dp[1]，$\cdots$，dp[n-1]中的最大值，下面想办法求解dp数组。 因为dp[i]要求必须是以A[i]作为末尾的连续序列的最大和，那么只有两种情况： 这个最大和的连续序列只有一个元素，即A[i]; 这个最大和的连续序列有多个元素，即从前面某处A[p]开始（p&lt;i），一直到A[i]结尾。 对于第一种情况，最大和就是A[i]本身。对于第二种情况，最大和是dp[i-1]+A[i]。于是得到状态转移方程 dp[i] = max\{A[i], dp[i-1] + A[i]\} 依此写出代码 1234567891011121314151617181920212223#include&lt;cstdio&gt;#include&lt;algorithm&gt;using namespace std;const int maxn = 10010;int A[maxn], dp[maxn];int main()&#123; int n; scanf("%d", &amp;n); for(int i = 0; i&lt; n; i++) scanf("%d", &amp;A[i]); dp[0] = A[0]; for(int i = 1; i &lt; n; i++) dp[i] = max(A[i], dp[i-1] + A[i]); int k = 0; for(int i = 1; i &lt; n; i++) &#123; if(dp[i] &gt; dp[k]) k = i; &#125; printf("%d\n", dp[k]); return 0;&#125; 不需要记录每个index的连续序列的最大和时可以写成以下简洁的形式： 123456789101112int maxSubArray(int* a, int n)//一维数组的最大连续子序列&#123; if (!a || n &lt;= 0) return 0; int themax = a[0], curmax = a[0]; for (int i = 1; i &lt; n; i++) &#123; curmax = max(a[i], curmax + a[i]); themax = max(themax, curmax); &#125; return themax;&#125; 2. 最大子矩阵和 已知矩阵的大小定义为矩阵中所有元素的和。第一行输入一个方阵的行数n，之后的n行输入方阵的每个元素，你的任务是找到最大的非空(大小至少是1 * 1)子矩阵。n0 -2 -7 09 2 -6 2-4 1 -4 1-1 8 0 -2的最大子矩阵是9 2-4 1-1 8这个子矩阵的大小是15。 直觉上来看，这个问题就是上述最大连续子序列和拓展到二维的情况。那么关键在于如何将两者联系起来。首先我们知道一定存在0$\le i \le j \le n-1$，最优解就在第i行和第j行之间，剩下的就是去确定两个列，我们可以通过遍历所有i, j的情况，针对一对确定的i, j，我们将每一列的[i, j]行之间的数各自相加。得到一个一维数组，$a[i][0] + $\cdots$ + a[j][0], $\cdots$, a[i][n-1] + $\cdots$ + a[j][n-1]$，而后对这个一维的和数组求最大连续子序列和。最后求出上述每个i, j的最大连续子序列和的最大值即为最大子矩阵和。得到一个$O(n^3)$的算法。代码如下：1234567891011121314151617181920212223242526const int MAXV = 110;int matrix[MAXV][MAXV];int tmp[MAXV]; //用于保存\[i, j\]行之间的数各自相加的一维数组void MaxSubMatrix()&#123; int n; scanf("%d", &amp;n); for (int i = 0; i &lt; n; i++) for (int j = 0; j &lt; n; j++) scanf("%d", &amp;matrix[i][j]); int tmpsum, themax = -1000000000; for (int i = 0; i &lt; n; i++) &#123; fill(tmp, tmp + n, 0);//每次移动i的时候tmp数组清零 for (int j = i; j &lt; n; j++)//遍历所有i, j的情况 &#123; for (int k = 0; k &lt; n; k++) &#123; tmp[k] += matrix[j][k];//重复利用tmp，因为对于i, j的遍历是先固定i，递增j，所以当j从p变为p+1时，可以利用$tmp_p$加上第p+1行的元素即可 &#125; tmpsum = maxSubArray(tmp, n); //上述的最大连续序列和 themax = max(themax, tmpsum); &#125; &#125; printf("%d\n", themax);&#125;]]></content>
      <categories>
        <category>千里怀思月在峰</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>DP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二分法——《算法笔记》]]></title>
    <url>%2F2020%2F01%2F06%2FThe-Binary-Search%2F</url>
    <content type="text"><![CDATA[1. 二分法适用范围二分法适用在一个严格单调序列中找到给定的某个数。 2. 二分法模板提要首先，在库中的有lower_bound()和upper_bound()两个函数，对于lower_bound()来说，它是寻找第一个满足条件“值大于等于x”的元素的位置；而对于upper_bound()来说，它是寻找第一个满足条件“值大于x”的元素的位置。对于此类寻找有序序列中第一个满足某条件的元素的位置的问题，有固定的模板，如下： 1234567891011121314151617//二分区间位左闭右闭的[left, right]，初值必须能覆盖解的所有可能取值int solve(int left, int right)&#123; int mid; while (left &lt; right) //对于[left, right]来说，left == right意味着找到唯一位置 &#123; mid = (left + right) / 2; if (条件成立) //条件成立，第一个满足条件的元素的位置&lt;=mid &#123; right = mid; //往左子区间[left, mid]寻找 &#125;else //条件不成立，则第一个满足该条件的元素的位置&gt;mid &#123; left = mid + 1;//往右子区间[mid+1, right]寻找 &#125; &#125; return left; //返回最终确定的位置&#125; 另外，如果想要寻找最后一个满足”条件C“的元素的位置，则可以先求第一个满足”条件!C“的元素的位置，然后将该位置减1即可。我们也可以把上述模板改成左开右闭的二分区间来实现。 1234567891011121314151617//二分区间位左开右闭的(left, right]，初值必须能覆盖解的所有可能取值，并且left比最小值小1int solve(int left, int right)&#123; int mid; while (left + 1 &lt; right) //对于(left, right]来说，left + 1== right意味着找到唯一位置 &#123; mid = (left + right) / 2; if (条件成立) //条件成立，第一个满足条件的元素的位置&lt;=mid &#123; right = mid; //往左子区间(left, mid]寻找 &#125;else //条件不成立，则第一个满足该条件的元素的位置&gt;mid &#123; left = mid ;//往右子区间(mid, right]寻找 &#125; &#125; return right; //返回最终确定的位置&#125; 3. 模板题3.1 木棒切割问题 给出长度已知的 n 根木棒，每根木棒的长度可能不同，现在希望通过切割它们得到至少 k 段长度相等的木棒（长度必须为整数），问这些长度相等的木棒最长能有多长。 根据题意可知，长度相等的木棒的长度 L 越长，可得到的长度相等的木棒的段数越少，因此这是一个单调的情况，可以二分木棒长度 L ，当最后刚好可得到 k 段木棒时的 L 值即为所求最大长度。 12345678910111213141516171819202122232425#include&lt;iostream&gt; using namespace std; int len[100]; int main() &#123; int n, k; cin &gt;&gt; n &gt;&gt; k; int right = 0; for (int i = 0; i &lt; n; i++) &#123; cin &gt;&gt; len[i]; if (len[i] &gt; right) right = len[i]; &#125; int left = 1; right++; while (left + 1 &lt; right) &#123; int mid = (left + right) / 2; int t = 0; for (int i = 0; i &lt; n; i++) t += len[i] / mid; if (t &lt; k) right = mid; else left = mid; &#125; cout &lt;&lt; right - 1 &lt;&lt; endl; return 0; 3.2 凸多边形的最大外接圆 给出 n 个线段的长度，试将他们头尾相接（顺序任意）地组合成一个凸多边形，使得该凸多边形的外接圆（即能使凸多边形的所有顶点都在圆周上的圆）的半径最大，求该最大半径。n 不超过 $10^{5}$，线段长度均不超过100，要求算法中不涉及坐标的计算。 对于要求的半径，当半径过小时，所有线段首位相连放在圆里面形成 n 个弦，弦所对的圆心角之和大于 360°，而当半径过大时，弦所对的圆心角之和小于360°。仔细思考，圆心角之和与对应半径是一个单调的关系，因此可以采用二分法解决。 123456789101112131415161718192021222324252627282930313233343536#include&lt;iostream&gt;#include&lt;math&gt; const double pi = acos(-1);const double eps = 1e-5; using namespace std; double m[100005]; int main() &#123; int n; double min = 101, max = 0, r; cin &gt;&gt; n; for (int i = 0; i &lt; n; i++) &#123; cin &gt;&gt; m[i]; if (m[i] &lt; min) min = m[i]; max += m[i]; &#125; min /= 2; while (min &lt; max) &#123; //左闭右闭 r = (min + max) / 2; double degree = 0; for (int i = 0; i &lt; n; i++) if (m[i] &gt; 2 * r) &#123; degree = pi + 1; //存在一条弦的长度大于两倍半径，这种情况不可能发生，所以当前的半径r过小，要到其右子区间进行搜索 break; &#125; else degree += asin(m[i] / 2 / r); //求得所有弦对应的圆心角之和 if (fabs(degree - pi) &lt; eps) break; //若圆心角之和等于360°，此时的r即为正确答案 else if (degree &gt; pi) min = r;//大于360°，要到其右子区间[r, max]进行搜索 else max = r; //否则，要到其左子区间[min, r]进行搜索。 &#125; cout &lt;&lt; r &lt;&lt; endl; return 0;&#125;]]></content>
      <categories>
        <category>三更有梦书为枕</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>二分</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[划分、解决、合并：分治法——《挑战程序设计竞赛》]]></title>
    <url>%2F2019%2F07%2F04%2FDivide-and-Conquer-Code-Challenging%2F</url>
    <content type="text"><![CDATA[4.6.1 数列上的分治法 给定一个1~n的排列a_0,a_1,\cdots, a_{n-1}，求这个数列中的逆序数对。 我们可以把一个大的数列A分成左右两个子数列B、C，那么数列A中所有的逆序对必定来自于以下三者其一：(1) i,j都属于左子数列B的逆序对(i,j);(2) i,j都属于右子数列C的逆序对(i,j);(3) i属于B而j属于C 对于这(1)和(2)，可以通过递归求得，对于(3)，我们可以对数列C中的每个数字，统计数列B中比它大的数字的个数，再把结果加起来就好。代码如下，复杂度为$O(n\log n)。12345678910111213141516171819202122232425262728293031323334typedef long long ll;//输入vector&lt;int&gt; A;ll merge_count(vector&lt;int&gt; &amp;a)&#123; int n = a.size(); if(n&lt;=1) return 0; ll cnt = 0; vector&lt;int&gt; b(a.begin(), a.begin()+n/2); vector&lt;int&gt; c(a.begin() + n/2, a.end()); cnt += merge_count(b); //(1) cnt += merge_count(c); //(2) //此时b和c就已经分别排好序了 //(3) int ai = 0, bi = 0, ci = 0; while(ai &lt; n)s &#123; if(bi &lt; b.size() &amp;&amp; (ci == c.size() || b[bi] &lt;= c[ci])) //ci == c.size()这个判断是，如果c数列已经全部找完了，剩下的全是b数列里的树，就不能看在有逆序数的存在 &#123; a[ai++] = b[bi++]; &#125; else &#123; cnt += (n/2 -bi); a[ai++] = c[ci++]; &#125; &#125; return cnt;&#125;void solve()&#123; printf("%lld\n", merge_count(A));&#125; 4.6.3 平面上的分治法]]></content>
      <categories>
        <category>三更有梦书为枕</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>分治法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[双端队列的运用——《挑战程序设计竞赛》]]></title>
    <url>%2F2019%2F07%2F04%2FThe-Usage-of-Deque-Code-Challenging%2F</url>
    <content type="text"><![CDATA[滑动最小值给定一个长度为n的数列a_0, a_1, \cdots, a_{n-1}和一个整数k。求数列$bi = \min{a_i,a{i+1},\cdots,a_{i+k-1} }(i=0,1,\cdots,n-k)。限制条件：$1\le k \le n \le 10^6$, $0 \le a_i \le 10^9$ 使用双端队列可以在O(n)时间内解决这个问题。最开始时双端队列为空，然后不断维护双端队列使它按照下面的顺序，存储用于计算后面的最小值a的元素的下标。 设双端队列从头部开始的元素的值为x_i，则x_i < x_{i+1}且a_i < a_{i+1}12345678910111213141516171819202122232425262728// 输入int n, k;int a[MAX_N];int b[MAX_N]; //保存答案的数组int deq[MAX_N];void solve()&#123; int s = 0, t = 0; //双端队列的头部和尾部 for(int i=0;i&lt;n;i++) &#123; while(s&lt;t &amp;&amp; a[deq[t-1]] &gt;= a[i]) t--; deq[t++] = i; if(i-k+1 &gt;= 0) &#123; b[i-k+1] = a[deq[s]]; if(deq[s] == i-k+1) &#123;//从双端队列的头部删除元素 s++; &#125; &#125; &#125; for(int i=0;i&lt;=n-k;i++) &#123; printf("%d%c",b[i],i==n-k?'\n':' '); &#125;&#125;]]></content>
      <categories>
        <category>三更有梦书为枕</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>双端队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[栈的运用——《挑战程序设计竞赛》]]></title>
    <url>%2F2019%2F07%2F03%2FThe-Usage-of-Stack-Code-Challenging%2F</url>
    <content type="text"><![CDATA[POJ 2559: Largest Rectangle in a Histogram 给定从左到右n个矩形，已知这此矩形的宽度都为1，长度不完全相等，为$h_1,h_2, \cdots, h_n$。这些矩形相连排成一排，求在这些矩形包括的范围内能得到的面积最大的矩形，打印出该面积。所求矩形可以横跨多个矩形，但不能超出原有矩形所确定的范围。$1\le n \le 100000$输入7 2 1 4 5 1 3 3输出8 从微软面试时学到的道理，拿到一个题目想想出最简单的算法，并能让这个算法能够成功地work。首先如果确定了长方形的左端点L和右端点R，那么最大可能的高度就是\min \{ h_i | L \le i]]></content>
      <categories>
        <category>三更有梦书为枕</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>栈</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络流解决问题——《挑战程序设计竞赛》]]></title>
    <url>%2F2019%2F07%2F01%2F%E7%BD%91%E7%BB%9C%E6%B5%81%E8%A7%A3%E5%86%B3%E9%97%AE%E9%A2%98%E2%80%94%E2%80%94%E3%80%8A%E6%8C%91%E6%88%98%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%AB%9E%E8%B5%9B%E3%80%8B%2F</url>
    <content type="text"><![CDATA[3.5.1 最大流求解最大流的Ford-Fulkerson算法的邻接表实现的例子如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include&lt;iostream&gt;#include&lt;queue&gt;#include&lt;vector&gt;#define MAX_V 10000#define INF 0x3f3f3f3fusing namespace std;//求解最大流问题的基础代码struct edge&#123;int to, cap, rev;&#125;;vector&lt;edge&gt; G[MAX_V]; //邻接表bool used[MAX_V];void add_edge(int from, int to, int cap)&#123; //第三个参数反向边是在G[to]中来自from边（反向边）的index。 G[from].push_back((edge)&#123;to,cap,G[to&#125;.size()&#125;); //因为刚刚from的邻接表中加了一个元素，所以要得到正确的反向边的index，就要减一 G[to].push_back((edge)&#123;from,0,G[from&#125;.size()-1&#125;) &#125;int dfs(int v,int t,int f)&#123; if(v == t)return f; used[v] = true; for(int i=0;i&lt;G[v].size();i++) &#123; edge &amp;e = G[v][i]; if(!used[e.to] &amp;&amp; e.cap &gt; 0) &#123; int d = dfs(e.to, t, min(f.e.cap)); G[e.to][e.rev].cap += d; return d; &#125; &#125; return 0;&#125;int max_flow(int s, int t)&#123; int flow = 0; for(;;) &#123; memset(used, 0, sizeof(used)); int f = dfs(s, t, INF); if(f == 0) return flow; flow += f; &#125;&#125; 记最大流的流量为F，那么该算法最多进行F次深度优先搜索，所以其复杂度为$O(F|E|)$。不过，这是一个很松的上界，达到这种最坏复杂度的情况几乎不存在。 3.5.7 应用问题（其中的应用多转化为二分图的最大匹配问题） Asteroids 有一个N*N的网格,该网格有K个小行星.你有一把武器,每次你使用武器发射光束可以攻击该网格特定行或列，从而清除行或列上的所有小行星.问你最少需要使用多少次武器能清除网格的所有小行星? 要破坏某个小行星，只能通过对应水平方向或者竖直方向的光束的攻击。利用攻击方法只有两种这一点，我们可以将问题按如下方法转换为图。 把光束当作图的顶点，而把小行星当作连接对应光束的边。这样转换之后，光束的攻击方案即对应一个顶点集合S，而要求攻击方案能够摧毁所有小行星，也就是图中的每条边都至少有一个属于S的端点，这样，这个问题就转化为了最小顶点覆盖问题。因为攻击方法只有两种，水平方向光束和竖直方向光束，那么这是一个二分图。最小覆盖问题通常是NP困难的，不过在二分图中等价于最大匹配问题，因而可以通过最大流算法高效地求解。 求解二分图的最大匹配问题可以看成是最大流问题的一种特殊情况。不妨对原图作如下变形：将原图中的所有无向边e改成有向边，方向从U到V（U和V是二分图中二分的顶点集），容量为1。增加源点s和汇点t，从s向所有顶点$u\in U$连一条容量为1的边，从所有的顶点$v\in V$向t连一条容量为1的边。 这样变形得到新图$G\prime$中的最大s-t流的流量就是原二分图G中最大匹配的匹配数，而U-V之间流量为正的边集合就是最大匹配。该算法的复杂度为$O(|V||E|)$。代码就是直接变形连线之后调用max_flow函数。下面给出基于匈牙利算法的二分图最大匹配数求解代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#include&lt;iostream&gt;#include&lt;queue&gt;#include&lt;vector&gt;#include&lt;cstring&gt;#define MAX_V 10000using namespace std;int V; //顶点数vector&lt;int&gt; G[MAX_V];int match[MAX_V];bool used[MAX_V];void add_edge(int u, int v)&#123; G[u].push_back(v); G[v].push_back(u);&#125;bool dfs(int v)&#123; used[v] = true; for(int i=0;i&lt;G[v].size();i++) &#123; int u = G[v][i], w = match[u]; if(w&lt;0 || !used[w] &amp;&amp; dfs(w)) &#123; match[v] = u; match[u] = v; return true; &#125; &#125; return false;&#125;int bipartite_matching()&#123; int res = 0; memset(match, -1, sizeof(match)); for(int v=0;v&lt;V;v++) &#123; if(match[v]&lt;0) &#123; memset(used, 0, sizeof(used)); if(dfs(v)) &#123; res++; &#125; &#125; &#125; return res;&#125;int main()&#123; int N, K; int R[10010], C[10010]; cin &gt;&gt;N&gt;&gt;K; for(int i=0;i&lt;K;i++) &#123; cin&gt;&gt;R[i]&gt;&gt;C[i]; &#125; V = N*2; for(int i=0;i&lt;K;i++) &#123; add_edge(R[i]-1, N+C[i]-1); &#125; cout&lt;&lt;bipartite_matching(); return 0;&#125;]]></content>
      <categories>
        <category>三更有梦书为枕</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>网络流</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[熟练掌握动态规划——《挑战程序设计竞赛》]]></title>
    <url>%2F2019%2F06%2F28%2F%E7%86%9F%E7%BB%83%E6%8E%8C%E6%8F%A1%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E2%80%94%E2%80%94%E3%80%8A%E6%8C%91%E6%88%98%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%AB%9E%E8%B5%9B%E3%80%8B%2F</url>
    <content type="text"><![CDATA[3.4.1 状态压缩DP可能在一些DP的递推关系式中，下表不是普通的整数，但是我们可以把它编码成一个整数，或者给它们定义一个全序关系并用二叉搜索树存储，从而可以记忆化搜索来求解。 旅行商问题：给定一个n个顶点组成的带权有向图的距离矩阵d(I,j)（INF表示没有边）。要求从顶点0出发，经过每个顶点恰好一次后再回到顶点0。问所经过的边的总权值的最小值是多少？$2\le n \le 15$$0\le d(i,j) \le 1000$ 所有可能的路线共有(n-1)!种，就算此题的n很小也不能列举。 假设现在已经访问过的顶点的集合（起点0当作还未访问过的顶点）为S，当前所在的顶点为v，用dp[s][v]表示从v出发访问剩余所有顶点最终返回顶点0这一段路径的权值总和的最小值，则有如下的递推关系式： dp[V][0] = 0 \\ dp[S][v] = \min\{dp[S\cup\{u\}][u]+d(v,u)|u\notin S\}这个递归式中，S是一个集合而不是普通的整数，对于集合我们可以把每一个元素的选取与否对应到一个二进制位里，从而把状态压缩到一个整数。 12345678910111213141516171819202122232425262728293031//输入#define INF 0x3f3f3f3fint n;int d[MAX_N][MAX_N];int dp[1&lt;&lt;MAX_N][MAX_N];int rec(int S,int v)&#123; if(dp[S][v] &gt;= 0) &#123; return dp[S][v]; &#125; if(S==(1&lt;&lt;n)-1 &amp;&amp; v==0)//已经访问过所有节点并回到0号点，所以剩余的权值为0 &#123; return dp[S][v] = 0; &#125; int res = INF; for(int u=0;u&lt;n;u++) &#123; if(!(S&gt;&gt;u &amp; 1)) &#123; res = min(res,rec(S|1&lt;&lt;u,u)+d[v][u]); &#125; &#125; return dp[S][v] = res;&#125;void solve()&#123; memset(dp,-1,sizeof(dp)); printf("%d\n",rec(0,0));&#125; 另外可以不用递归而从全序排列字典序倒叙来循环求解。而像这样针对集合的DP我们一般叫状态压缩DP。 铺砖问题给定$n\times m$的格子，每个格子被染成了黑色或者白色。现在要用$1\times 2$的砖块覆盖这些格子，要求块与块之间不互相重叠，且覆盖了所有白色的格子，但不覆盖任意一个黑色格子。求一共有多少种覆盖方法，输出方案数对M取余后的结果。 这道题的想法有点难理解昂。 索性不强行理解了，咱们根据代码来理解。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include&lt;iostream&gt;using namespace std;bool M[17][17]; //存初始化格子的颜色/*dp[used]:=(这个used是第二个参数，m个格子的枚举)从起始位置到当前的m个格子，都被覆盖的所有可能性*/int dp[2][1&lt;&lt;15]; int main()&#123; int n,m,mod; while(cin&gt;&gt;n&gt;&gt;m &amp;&amp;n&amp;&amp;m)&#123; cin&gt;&gt;mod; memset(dp,0,sizeof(dp)); for(int i=0;i&lt;n;i++)&#123; for(int j=0;j&lt;m;j++)&#123; //从0开始后面会好操作一点 char input; cin&gt;&gt;input; if(input=='.') M[i][j]=0; //0:白 1：黑，将0全变为1 else M[i][j]=1; &#125; &#125; int *now=dp[0],*nex=dp[1]; now[0]=1; for(int i=n-1;i&gt;=0;i--)&#123; //从n-1 开始会方便二进制表示状态，我也不知道为什么就方便起来了 for(int j=m-1;j&gt;=0;j--)&#123; for(int used=0;used&lt; (1&lt;&lt;m) ;used++)&#123; //遍历状态,这里反过来表示 /*为什么要遍历呢？看下面竖着放的代码有一个： res+=now[used|1&lt;&lt;j];横着放也是一样，是从当前used的状态中新置1一个比特，因此需要！*/ if(used&gt;&gt;j &amp; 1 || M[i][j])&#123; //假如这个位置被用了或者是1 不用填 就是不需要被覆盖呢，那么下一个处理的格子的可能性就等于这一个格子的可能性，这个格子的可能性也就对于这个格子没被覆盖的可能性，毕竟要递推过来的嘛。 nex[used]=now[used &amp; ~(1&lt;&lt;j)]; //把第j这个位置上的比特置为0，也就是这个格子没被覆盖的意思 &#125; else&#123; int res=0;/*对于一个白色格子，那么覆盖它的可能性就等于横放的可能性（右边和它一起被覆盖）+竖放可能性（下边和它一起被覆盖）。*/ if(j+1&lt;m &amp;&amp; !(used&gt;&gt;(j+1)&amp;1) &amp;&amp; !M[i][j+1])&#123; //横着放 /*j+1的原因是上面for循环是j--， &lt;- 这个方向遍历的；这个if的!(used&gt;&gt;(j+1)&amp;1) 条件是说j+1这个位置，也就是右边这个格子，在当前枚举的这种状态下没有被覆盖，因为只有右边这个格子没被覆盖，这下横着放，占了右边格子和当前格子才行，那为什么不能是j-1呢？因为j-1还没处理到，我的理解是不能影响后面还未处理的格子。那么这种可能性就等于右边和它一起被覆盖的可能性。*/ res+=now[used|1&lt;&lt;(j+1)]; &#125; if(i+1&lt;n &amp;&amp; !M[i+1][j] )&#123; //竖着放， 同理了，那么这种可能性就等于下边的格子和它一起被覆盖的可能性。 res+=now[used|1&lt;&lt;j]; &#125; nex[used]=res%mod; &#125; &#125; swap(now,nex); &#125; &#125; cout&lt;&lt;now[0]&lt;&lt;endl; &#125; return 0; &#125;/*3 3 100. . .. X .. . .*/]]></content>
      <categories>
        <category>三更有梦书为枕</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>DP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[树状数组——《挑战程序设计竞赛》]]></title>
    <url>%2F2019%2F06%2F28%2F%E6%A0%91%E7%8A%B6%E6%95%B0%E7%BB%84%E2%80%94%E2%80%94%E3%80%8A%E6%8C%91%E6%88%98%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%AB%9E%E8%B5%9B%E3%80%8B%2F</url>
    <content type="text"><![CDATA[树状数组(Binary Indexed Tree(B.I.T), Fenwick Tree)是一个查询和修改复杂度都为log(n)的数据结构。给一个初始值全为0的数列，$a_1, a_2, \cdots, a_n$，树状数组可以进行如下操作： 给定i，计算$a_1+a_2+\cdots+a_i$ 给定i和x，执行$a_i+=x$]]></content>
      <categories>
        <category>三更有梦书为枕</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>树状数组</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线段树——《挑战程序设计竞赛》]]></title>
    <url>%2F2019%2F06%2F27%2F%E7%BA%BF%E6%AE%B5%E6%A0%91%E2%80%94%E2%80%94%E3%80%8A%E6%8C%91%E6%88%98%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%AB%9E%E8%B5%9B%E3%80%8B%2F</url>
    <content type="text"><![CDATA[3.3.1 线段树线段树是一种二叉搜索树，它将一个区间划分成一些单元区间，每个单元区间对应线段树中的一个叶结点（并非完全二叉树！！！）。实际应用时一般还要开4N的数组以免越界。 线段树的构造代码如下： 1234567891011121314151617181920212223242526272829303132#include &lt;iostream&gt;using namespace std; const int maxind = 256;int segTree[maxind * 4 + 10];int array[maxind]; /* 构造函数，得到线段树 *///区间为[begin, end]void build(int node, int begin, int end) &#123; if (begin == end) segTree[node] = array[begin]; /* 只有一个元素,节点记录该单元素 */ else &#123; /* 递归构造左右子树 */ build(2*node, begin, (begin+end)/2); build(2*node+1, (begin+end)/2+1, end); /* 回溯时得到当前node节点的线段信息，保存最小值 */ if (segTree[2 * node] &lt;= segTree[2 * node + 1]) segTree[node] = segTree[2 * node]; else segTree[node] = segTree[2 * node + 1]; &#125; &#125; int main()&#123; array[0] = 1, array[1] = 2,array[2] = 2, array[3] = 4, array[4] = 1, array[5] = 3; build(1, 0, 5); return 0;&#125; 线段树的主要操作包括区间查询（查询给定区间的最小值）和给定节点的更新。 区间查询 int query(int node, int begin, int end, int left, int right);时间复杂度$O(\log n)$。（其中node为当前查询节点，begin,end为当前节点存储的区间，left,right为此次query所要查询的区间，实际上我们只想要查询[left, right)的最小值其他的参数是为了计算方便传入的）线段树区间查询的主要思想是把所要查询的区间[a,b]划分为线段树上的节点，然后将这些节点代表的区间合并起来得到所需信息：（对于节点存储对应区间最小值的线段树来说） 如果所查询区间和当前节点对应的区间完全没有交集，那么就返回一个不影响答案的值 如果所查询的区间包含了当前节点对应的区间，那么就返回当前节点的值 以上两种情况都不满足的话，就对两个儿子递归处理，返回两个结果中的较小值123456789101112131415161718192021222324252627int query(int node, int begin, int end, int left, int right) &#123; int p1, p2; /* 查询区间和要求的区间没有交集 */ if (left &gt; end || right &lt; begin) return -1; /* if the current interval is included in */ /* the query interval return segTree[node] */ if (begin &gt;= left &amp;&amp; end &lt;= right) return segTree[node]; /* compute the minimum position in the */ /* left and right part of the interval */ p1 = query(2 * node, begin, (begin + end) / 2, left, right); p2 = query(2 * node + 1, (begin + end) / 2 + 1, end, left, right); /* return the expect value */ if (p1 == -1) return p2; if (p2 == -1) return p1; if (p1 &lt;= p2) return p1; return p2; &#125; 单节点更新1234567891011121314151617void Updata(int node, int begin, int end, int ind, int add)/*单节点更新*/ &#123; if( begin == end ) &#123; segTree[node] += add; return ; &#125; int m = ( left + right ) &gt;&gt; 1; if(ind &lt;= m) Updata(node * 2,left, m, ind, add); else Updata(node * 2 + 1, m + 1, right, ind, add); /*回溯更新父节点*/ segTree[node] = min(segTree[node * 2], segTree[node * 2 + 1]); &#125; Crane 有n根长度不尽相同的棍子，初始时它们首尾垂直相连，标号为1—n，第一根棍子的下端坐标为(0,0),上端坐标为(0,len[1]),其余棍子依次类推。接下来执行C此旋转，每次输入一个编号num和角度rad，使得第num根棍子和第num+1跟棍子间的逆时针角度变为rad度，求每次旋转后第n根棍子端点的坐标。 解题思路源于[https://www.cnblogs.com/staginner/archive/2012/04/07/2436436.html] 如果我们将其中某一个线段旋转β角，那么这个线段上方的所有线段都会旋转β角，这就很类似线段树中的对区间加上一个常数的问题了，于是不妨向着线段树的思路去想。 接下来一个问题就是β角是相对于谁的，换句话说我们所谓的每个线段都会旋转β角，那么是绕谁旋转的？实际上，如果我们局限于把线段的旋转就会看成是绕某个定点的，这个点就是我们旋转的线段和它下面那个不动的线段的交点，再这样想下去我们就没法处理了，因为每个旋转操作所绕的定点不是唯一的，我们没办法把所有的旋转操作都统一到一起，那么我们就没办法把旋转操作叠加，这样就没法使用线段树了。 但如果换个思路的话，实际上β角还等于这个线段旋转后所在的直线和未旋转前所在的直线的夹角，而直线的夹角是可以用向量的夹角表示的，如果我们把线段看成一个向量的话那么β角就是这个向量旋转的角度。如果这么看的话，所有的旋转操作就可以统一到一起了，也可以叠加了，因为这样不会局限于绕哪个定点，只需要把向量自身旋转一下就OK。 那么我们维护下面两个值： 把对应线段集合转到垂直方向（也就是整体旋转，让第一条线段垂直之后，注意并不是单独旋转第一条线段），从第一条线段的起点指向最后一条线段的终点的向量。 （如果该节点有儿子节点）两个儿子节点对应的部分连接之后，右儿子需要转动的角度（因为s和s+1的角度改变，如果s在左儿子中，那么在全局坐标系内，右儿子也会相应的需要旋转改变坐标） 也就是说，如果节点i表示的向量是$vx_i, vy_i$，角度是$ang_i$，两个儿子节点是chl和chr，那么就有： vx_i = vx_{chl}+(\cos(ang_i)\times vx_{chr} - \sin(ang_i)\times vy_{chr}) \\ vy_i = vy_{chl}+(\sin(ang_i)\times vx_{chr} + \cos(ang_i)\times vy_{chr})对向量旋转的解释可参照[https://www.2cto.com/kf/201610/556382.html] 这样每次更新可在$O(\log n)$的时间内完成。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586#include &lt;iostream&gt;#include&lt;stdio.h&gt;#include&lt;math.h&gt;using namespace std; const int maxind = 10010;const double PI = acos(-1.0);int array[maxind];int S[maxind], A[maxind];double vx[maxind * 4 + 10], vy[maxind * 4 + 10];double ang[maxind * 4 + 10];double prv[maxind];/* 构造函数，得到线段树 */void build(int node, int begin, int end) &#123; ang[node] = vx[node] = 0; if (begin == end) cin&gt;&gt;vy[node]; else &#123; /* 递归构造左右子树 */ build(2*node, begin, (begin+end)/2); build(2*node+1, (begin+end)/2+1, end); /* 回溯时得到当前node节点的线段信息，保存最小值 */ vy[node] = vy[2*node] + vy[2*node+1]; &#125; &#125; void change(int s, double a, int node, int begin, int end)&#123; if(s&lt;begin) return; else if(begin == s &amp;&amp; begin == end) //区间只用一个棍子的情况 &#123; return; &#125; else if(s &lt; end)&#123; int chl = node*2, chr = node*2+1; int mid = (begin+end)&gt;&gt;1; change(s,a,chl,begin,mid); change(s,a,chr,mid+1,end); if(s&lt;=mid) ang[node]+=a; //(1, pi/2, 1, 1, 2) 这时mid就为1，s也为1，是最细粒度的情况，需要给ang赋值 /* s&lt;=m，s在左儿子中，那么右边的线段也会因为s和s+1的角度改变而全局的坐标发生改变，当前这个节点node左右儿子之间的角度也会发生变化（变化a），所以是if(s&lt;=mid) ang[node]+=a，之后根据变化后的角度，改变当前node维护的vx和vy */ double sine = sin(ang[node]), cosine = cos(ang[node]); //cout&lt;&lt;"sine is "&lt;&lt;sine&lt;&lt;", cosine is "&lt;&lt;cosine&lt;&lt;endl; vx[node] = vx[chl] + (cosine * vx[chr] - sine * vy[chr]); vy[node] = vy[chl] + (sine * vx[chr] + cosine * vy[chr]); &#125;&#125; int main()&#123; int N, C; int t=0; while(scanf("%d%d", &amp;N, &amp;C) == 2) &#123; build(1,1,N); for(int i=0;i&lt;C;i++) &#123; cin&gt;&gt;S[i]&gt;&gt;A[i]; &#125; if(t++) printf("\n"); for(int i=0;i&lt;=N;i++) prv[i] = PI; for(int i=0;i&lt;C;i++) &#123; int s = S[i]; double a = A[i] / 180.0 *PI; change(s,a-prv[s], 1, 1, N); prv[s] = a; printf("%.2f %.2f\n", vx[1], vy[1]); &#125; &#125; return 0;&#125; 线段树区间更新A Simple Problem with Integers 给N个数$A_1, A_2, … , A_n$. 你需要处理两种操作，一种操作是在一个区间上每个数都增加一个数，别一种操作是查询一个区间所有数的和 以目前仅有的单节点更新很难高效滴实现对一个区间同时加一个值，所以我们改进线段树，对每个节点我们维护一下两个数据： a. 给这个节点对应的区间内的所有元素共同加上的值 b. 在这个节点对应的区间中除去a（a就是上面那个a）之外所有其他值的和如果对于父节点同时加上了一个值，那么这个值就不会在儿子节点被重复考虑，而是在递归计算和的时候再把这一部分的值加到结果中。这样无论是同时加一个值还是查询一段的和复杂度都是$O(\log n)$。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394#include &lt;iostream&gt;using namespace std;const int maxn=100000+5;typedef long long LL;LL sum[maxn&lt;&lt;2], add[maxn&lt;&lt;2];int a[maxn];void PushUp(int root)&#123; sum[root] = sum[root&lt;&lt;1] + sum[root&lt;&lt;1|1];&#125;void Build(int l, int r, int root)&#123; add[root] = 0; if(l == r) &#123; scanf("%I64d",&amp;sum[root]); return; &#125; int m=(l+r)&gt;&gt;1; Build(l,m,root&lt;&lt;1); Build(m+1,r,root&lt;&lt;1|1); PushUp(root);&#125;void PushDown(int root,int m)&#123; if(add[root]) &#123; add[root&lt;&lt;1]+=add[root]; add[root&lt;&lt;1|1]+=add[root]; sum[root&lt;&lt;1]+=(m-(m&gt;&gt;1))*add[root]; sum[root&lt;&lt;1|1]+=(m&gt;&gt;1)*add[root]; add[root]=0; &#125;&#125;void Update(int L,int R,int c,int l,int r, int root)&#123; if(L&lt;=l&amp;&amp;R&gt;=r) &#123; add[root]+=c; sum[root]+=(LL)c*(r-l+1); return; &#125; PushDown(root,r-l+1); int m=(l+r)&gt;&gt;1; if(L&lt;=m) Update(L,R,c,l,m,root&lt;&lt;1); if(R&gt;m) Update(L,R,c,m+1,r,root&lt;&lt;1|1); PushUp(root);&#125;LL Query(int L,int R,int l, int r,int root)&#123; if(L&lt;=l &amp;&amp; R&gt;=r) return sum[root]; PushDown(root,r-l+1); //在查询和的时候在调用PushDown，把保存在add数组中的值加到sum数组的结果之中。 int m=(l+r)&gt;&gt;1; LL res = 0; if(L&lt;=m) res += Query(L,R,l,m,root&lt;&lt;1); if(R&gt;m) res += Query(L,R,m+1,r,root&lt;&lt;1|1); return res; &#125;int main()&#123; int m,n; scanf("%d%d",&amp;n,&amp;m); Build(1,n,1); while(m--) &#123; char s[5]; int a,b,c; scanf("%s",s); if(s[0]=='Q') &#123; scanf("%d%d",&amp;a,&amp;b); printf("%I64d\n",Query(a,b,1,n,1)); &#125; else &#123; scanf("%d%d%d",&amp;a,&amp;b,&amp;c); Update(a,b,c,1,n,1); &#125; &#125; return 0;&#125; 3.3.3 分桶法和平方分割分桶法是把一排物品或者平面分成桶，每个桶分别维护自己内部的信息，以达到高效计算的目的。 其中平方分割是把排成一排的n个元素每个$\sqrt{n}$个分在一个桶内进行维护的方法的统称。这样的分割方法可以使区间的操作的复杂度降至$O(\sqrt{n})$。 水平分割法相比于线段树实现上要简单，单多数情况下线段树会比水平分割快。 K-th Number 给出一个长度为n的序列a，给出m次查询，每次查询区间[l,r]中第k大的数是什么]]></content>
      <categories>
        <category>三更有梦书为枕</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>线段树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用技巧精选(一)——《挑战程序设计竞赛》]]></title>
    <url>%2F2019%2F06%2F27%2F%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7%E7%B2%BE%E9%80%89%E2%80%94%E2%80%94%E3%80%8A%E6%8C%91%E6%88%98%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%AB%9E%E8%B5%9B%E3%80%8B%2F</url>
    <content type="text"><![CDATA[3.2 常用技巧精选（一）3.2.1 尺取法Subsequence 给出N个数字：a_0, a_1, \cdots, a_{n-1}，每个数字不大于10000，给出一个整数S，在N个数字中挑选出连续子序列，使这个子序列和大于或等于S。请问这个连续的子序列长度的最小值。 我们设以a_s开始总和最初大于S时的连续子序列为a_s+\cdots+a_{t-1}，这时 a_{s+1}+\cdots+a_{t-2} < a_s+\cdots+a_{t-2} < S所以从$a_{s+1}$开始总和最初超过S的连续子序列如果是$a_{s+1}+\cdots+a_{t\prime-1}$的话，必然有$t\le t\prime$。利用这一性质便可以设计如下算法：(1) 以s=t=sum=0初始化。(2) 只要依然有sum&lt;S，就不断将sum增加$a_t$，并将t增加1.(3) 如果(2)中无法满足sum$\ge$S则终止。否则的话，更新res=min(res,t-s)。(4) 将sum减去$a_s$，s增加1然后回到(2)。 对于这个算法，因为t最多变化n次，因此只需O(n)的复杂度就可以求解这个问题了。1234567891011121314151617181920212223242526272829303132333435#include&lt;iostream&gt;using namespace std;int a[100010];int main()&#123; int t; cin&gt;&gt;t; while(t--&gt;0) &#123; int n,S; cin&gt;&gt;n&gt;&gt;S; for(int i=0;i&lt;n;i++) &#123; cin&gt;&gt;a[i]; &#125; int res = n + 1; int s = 0, t = 0, sum = 0; for(;;) &#123; while(t&lt;n &amp;&amp; sum&lt;S) &#123; sum += a[t++]; &#125; if(sum &lt; S) break; res = min(res,t-s); sum -= a[s++]; &#125; if(res &gt; n) &#123; res = 0; &#125; cout&lt;&lt;res&lt;&lt;endl; &#125;&#125; 3.2.2 反转（开关问题）Face The Right Way N头牛排成一列，头要么朝前要么朝后，现在要求确定一个连续反转牛头的连续区间，区间长度为K，要使得所有牛都朝前，且反转次数M尽可能小。求出最小的操作数M和对应的最小的K。 首先，交换区间反转顺序的先后对结果毫无影响。此外，可以知道对同一个区间进行两次以上的反转是多余的，由此，问题就转化成了求需要被反转的区间的集合。于是我们先考虑一下最左端的牛。包含这头牛的区间只有一个，因此如果这头牛面朝前方，我们就能知道这个区间不需要反转。反之，如果这头牛面朝后方，对应的区间就必须进行反转了。而且在此之后这个最左的区间就再也不需要考虑了。这样一来，通过首先考虑最左端的牛，问题的规模就缩小了1。不断的重复下去，就可以无需搜索求出最少所需的反转次数了。 然而，我们需要遍历K，对于每个K我们都要从最左端开始考虑N头牛的情况，最坏情况需要进行N-K+1次的反转操作，而每次操作又要反转K头牛，所以总的复杂度是$O(N^3)$。 对于区间反转部分进行优化：优化的方法是计算第i头牛是否要翻转的时候，只需要知道第i-k+1头到第i头之间的翻转次数，那么维护这个次数即可。 f[i]:=区间[i,i+K-1]进行了反转的话则为1，否则为0这样，在考虑第i头年时，如果$\sum_{j=i-K+1}^{i-1}f[j]$为奇数的话，则这头牛的方向与起始方向是相反的，否则方向不变。依据如下公式 \sum_{j=(i+1)-K+1}^{i}f[j] = \sum_{j=i-K+1}^{i-1}f[j]+f[i]-f[i-K+1]使用尺取法，每次向右移动一格，需要看看左边出去的那格（第i-k格）是翻转了没有，维护好f数组即可。这样扫一遍的复杂度是$O(n)$，那么总复杂度就是$O(n^2)$。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556const int M = 5100;int n;int dir[M];int f[M];int calc(int K)&#123; memset(f,0,sizeof(f)); int res = 0; int sum = 0; //当前维护的f的和 for(int i=0;i+K&lt;=n;i++) &#123; if((dir[i]+sum) % 2 != 0) //最前端的牛面朝后方 &#123; res++; f[i] = 1; &#125; sum += f[i]; if(i-K+1&gt;=0) &#123; sum -= f[i-K+1]; &#125; &#125; // 检查剩下的牛，从n-K+1开始，是否有面朝后方的情况 for(int i=n-K+1;i&lt;n;i++) &#123; if((dir[i]+sum)%2!=0) //无解 &#123; return -1; &#125; if(i-K+1&gt;=0) sum-=f[i-K+1]; &#125; return res;&#125;int main()&#123; cin&gt;&gt;n; for(int i=0;i&lt;n;i++) &#123; char c; cin&gt;&gt;c; if(c=='B') &#123; dir[i]=1; &#125; else &#123; dir[i]=0; &#125; &#125; solve(); return 0; &#125; Fliptile 有一个m*n的棋盘，每个格子上是0或1，每次可以对一个格子做一次翻转操作，将被操作的格子和上下左右4个格子的0/1翻转。问做少做多少次翻转可以将所有格子翻转成0，输出翻转方案。没有方案时输出“IMPOSSIBLE”。$1\le m, n \le 15$ 回顾一下前情，在之前POJ3276问题中，让最左边的牛反转的方法只有1种，可以直接判断当前的方案是否可行。然而，同样的方法在此题中不能行得通。不妨看最左上角的格子，这里除了翻转(1,1)之外，翻转(1,2)和(2,1)都可以把这个格子翻转，所以之前的直接确定的方法在此行不通。 值得注意的一点是这题中的m，n特别小，简直就是在明示可以有某种枚举的方法。因为本题格子间的状态都是互相影响的，只能通过枚举第一行，逐行往下搜，如何搜索：如果从上到下搜索，当前行是否需要反转取决于上一行的状态，通过翻转当前行使上一行为0，而不是通过上一行翻转为0后，看当前行的状态判断自己是否需要翻转，否则还会继续影响上一行。意思就是不是在当前行中的翻转操作不是为了让当前行中的所有格子都为0，而是要让上一行的所有格子都为0。所以我们可以通过枚举第一行所有的状态，从第二行开始确定翻转状态，直到最后一行结束，如果可以保证最后一行都是0，那么方案可行，否则重新定义第一行的状态，继续搜索，找出使反转次数最少的方案。 3.2.4 折半枚举（双向搜索） 超大背包问题有重量和价值分别为$w_i, v_i$的n个物品，从这些物品中挑选总重量不超过W的物品，求所有挑选方案中价值总和的最大值限制条件：$1 \le n \le 40, 1\le w_i, v_i \le 10^{15}, 1 \le W \le 10^{15} $ 所谓超大背包问题，W的最大值是$10^15$，使用DP来求解背包问题的复杂度是$O(nW)$，因此不能解决这里的问题。但是n比较小，依此寻求枚举解法。 从所有物品里挑选的全排列的数量为$2^n$中，在这里虽然n很小，但是$2^40$依然顶不住，所以我们想到折半枚举，把物品拆成两半再枚举。我们把前半部分中枚举出的重量和价值总和记为$w_1、 v_1$。这样在后半部分寻找总重$w_2\le W-w_1$时使$v_2$最大的选取方法就好了。 接着我们考虑从枚举得到的$(w_2,v_2)$的集合中高效寻找$max{v_2|w_2 \le W\prime}$的方法。在代码中当然使用pair这个数据结构来维护重量和价值对，那么我们根据重量排序之后可以排除所有$w_2[i] \le w_2[j]$并且$v_2[i] \ge v_2[j]$的j。接下来只要寻找$w_2[i] \le W\prime$的最大的i就可以了。使用二分搜索完成，剩余元素的个数为M（$M\le 2^{(n/2)}$）的话，一次搜素需要$O(\log M)$，所以这个算法的总复杂度是$O(2^{(n/2)}n)。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455typedef long long ll;//输入int n;ll w[MAX_N], v[MAX_N];ll W;pair&lt;ll,ll&gt; ps[1 &lt;&lt; (MAX_N / 2)]; // (重量，价值)对void solve()&#123; //枚举前半部分 int n2 = n / 2; for(int i=0;i&lt;1&lt;&lt;n2;i++) &#123; ll sw = 0, sv = 0; for(int j=0;j&lt;n2;j++) &#123; if(i&gt;&gt;j &amp; 1) &#123; sw += w[j]; sv += v[j]; &#125; &#125; ps[i] = make_pair(sw,sv); &#125; //去除多余元素 sort(ps, ps+(1&lt;&lt;n2)); int m = 1; for(int i=1;i&lt;1&lt;&lt;n2;i++) &#123; if(ps[m-1].second &lt; ps[i].second) &#123; ps[m++] = ps[i]; &#125; &#125; //枚举后半部分并求解 ll res = 0; for(int i=0;i&lt;1&lt;&lt;(n-n2);i++) &#123; ll sw = 0, sv = 0; for(int j=0;j&lt;n-n2;j++) &#123; if(i&gt;&gt;j &amp; 1) &#123; sw += w[n2+j]; sv += v[n2+j]; &#125; &#125; if(sw &lt;= W) &#123; ll tv = (lower_bound(ps,ps+m,make_pair(W-sw,INF))-1)-&gt;second; res = max(res, sv + tv); &#125; &#125; printf("%lld\n", res);&#125; 3.2.5 坐标离散化 w*h的格子上画了n条垂直或水平的宽度为1的直线。求出这些线将格子划分成了多少个区域。1&lt;=w,h&lt;=1000000. 1&lt;=n&lt;=500样例：w = 10, h = 10, n = 5x1 = {1, 1, 4, 9, 10}x2 = {6, 10, 4, 9, 10}y1 = {4, 8, 1, 1, 6}y2 ={4, 8, 10, 5, 10} 理解了半天这个样例，意思是( (x1, y1), (x2, y2) )是一条直线。一般求解被分割出的区域的个数使用图的遍历如DFS和BFS算法，需要$w\times h$，但是这个问题中w和h最大为1000000，所以没办法创建出$w\times h$的数组，所以需要使用坐标离散化技巧。 数组里只需要存储有直线的行列以及前后的行列就足够了，这样的话大小最多$6n \times 6n$就足够了（x1的自己、前后，3个，x2的自己、前后，3个，因此则有 $6n \times 6n$）。创建出数组并利用搜索求出区域的个数。（区域可能很大，所以用递归函数实现的话可能会栈溢出，所以在下列代码实现中不用DFS而用BFS） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172//输入int W, H, Nint X1[MAX_N], X2[MAX_N], Y1[MAX_N], Y2[MAX_N];bool fld[MAX_N*6][MAX_N*6];int dx[4]=&#123;-1,0,0,1&#125;, dy[4]=&#123;0,-1,1,0&#125;;int compress(int *x1, int *x2; int w)&#123; vector&lt;int&gt; xs; for(int i=0;i&lt;N;i++) &#123; for(int d = -1;d&lt;=1;d++) &#123; int tx1 = x1[i] + d, tx2 = x2[i] + d; if(1&lt;=tx1 &amp;&amp; tx1&lt;=w) xs.push_back(tx1); if(1&lt;=tx2 &amp;&amp; tx2&lt;=w) xs.push_back(tx2); &#125; &#125; sort(xs.begin(),xs,end()); xs.erase(unique(x.begin(),xs.end()),xs.end()); //去重 for(int i=0;i&lt;N;i++) //重新建立x1, x2的值 &#123; x1[i] = find(xs.begin(),xs.end(),x1[i]) - xs.begin(); x2[i] = find(xs.begin(),xs.end(),x2[i]) - xs.begin(); &#125; return xs.size(); //因为有去重，所以不慌&#125;void solve()&#123; W = compress(X1,X2,W); H = compress(Y1,Y2,H); memset(fld, 0, sizeof(fld)); for(int i=0;i&lt;N;i++) &#123; for(int y=Y1[i];y&lt;=Y2[i];y++) &#123; for(int x=X1[i];x&lt;=X2[i];x++) &#123; fld[y][x] = true; &#125; &#125; &#125; int ans = 0; for(int y=0;y&lt;H;y++) &#123; for(int x=0;x&lt;W;x++) &#123; if(fld[y][x]) continue; ans++; queue&lt;pair&lt;int, int&gt; &gt; que; que.push(make_pair(x,y)); while(!que.empty()) &#123; int sx = que.front().first(), sy = que.front().second(); que.pop(); for(int i=0;i&lt;4;i++) &#123; int tx = sx + dx[i], ty = sy + dy[i]; que.push(make_pair(tx,ty)); fld[ty][tx] = true; &#125; &#125; &#125; &#125; printf("%d\n", ans);&#125;]]></content>
      <categories>
        <category>三更有梦书为枕</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>尺取法</tag>
        <tag>开关问题</tag>
        <tag>坐标离散化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数学问题的解题窍门——《挑战程序设计竞赛》]]></title>
    <url>%2F2019%2F06%2F26%2F%E6%95%B0%E5%AD%A6%E9%97%AE%E9%A2%98%E7%9A%84%E8%A7%A3%E9%A2%98%E7%AA%8D%E9%97%A8%E2%80%94%E2%80%94%E3%80%8A%E6%8C%91%E6%88%98%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%AB%9E%E8%B5%9B%E3%80%8B%2F</url>
    <content type="text"><![CDATA[2.6.2 有关素数的基础算法埃氏筛法 素数的个数：给定整数n，请问n以内有多少个素数？ 首先将2到n范围内的所有整数写下来，其中最小的数字2是素数，将表中所有2的倍数都划去，表中剩余的最小数字是3，它不能被更小的数整除，所以是素数，再将所有3的倍数划去。以此类推：123456789101112131415161718int prime[MAX_N]; //第i个素数bool is_prime[MAX_N + 1];//返回n以内素数的个数int sieve(int n)&#123; int p = 0; for(int i=0;i&lt;=n;i++)is_prime[i]=true; is_prime[0] = is_prime[1] = false; for(int i=2;i&lt;=n;i++) &#123; if(is_prime[i]) &#123; prime[p++] = i; for(int j=2*i;j&lt;=n;j+=i) is_prime[j] = false; &#125; &#125; return p;&#125; 埃氏筛法的复杂度仅有$O(n\log\log n)$。 2.6.4快速幂运算[https://onlinejudge.org/index.php?option=com_onlinejudge&amp;Itemid=8&amp;category=12&amp;page=show_problem&amp;problem=947]: “Carmicheal Numbers” 我们把对任意的1&lt;x&lt;n都有$x^n\equiv x(mod n)$成立的合数n称为Carmichael Numbers。对于给定的整数n，判断它是不是Carmichael Number。 考虑加速幂运算的方法，如果$n=x^k$，可以将其表示为 x^n = ((x^2)^2) \cdots只要做k次平方运算就可以轻松求得。因此我们可以先将n表示成2的幂次的和： n = 2^{k_1} + 2^{k_2} + 2^{k_2} + \cdots就有： x^n = x^{2^{k_1}}x^{2^{k_2}}x^{2^{k_3}}\cdots只要依此求$x^{2^{k_i}}$，最终可以得到$O(\log n)$的计算幂运算的复杂度。123456789101112typedef long long ll;ll mod_pow(ll x, ll n, ll mod)&#123; ll res = 1; while(n&gt;0) &#123; if(n&amp;1) res = res * x % mod; x = x * x % mod; n &gt;&gt;= 1; &#125; return res;&#125; 在遍历n，即可得解，复杂度为$O(n\log n)$。]]></content>
      <categories>
        <category>三更有梦书为枕</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>素数相关</tag>
        <tag>快速幂</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图算法——《挑战程序设计竞赛》]]></title>
    <url>%2F2019%2F06%2F25%2F%E5%9B%BE%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94%E3%80%8A%E6%8C%91%E6%88%98%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%AB%9E%E8%B5%9B%E3%80%8B%2F</url>
    <content type="text"><![CDATA[2.5.2 图的表示邻接表可以直接用如下代码表示： 123456vector&lt;int&gt; graph[MAX_V] 来表示邻接表 /*边上有属性的情况： struct edge &#123;int to, cost;&#125; vector&lt;edge&gt; G[MAX_V];*/ 2.5.4 最短路问题1. 单源最短路问题Bellman-Ford算法记从起点s出发到顶点i的最短距离为d[i]，则下述等式成立： d[i] = \min\{d[j]+(从j到i的边的权值| e=(j,i)\in E\}设初值d[s]=0, d[i]=INF，再不断使用这条递推关系式更新d的值。只要图中不存在负圈，这样的更新操作就是有限的。 123456789101112131415161718192021222324struct edge &#123;int from, to, cost&#125;;edge es[MAX_E]; //边int d[MAX_V];int V, E;void shortest_path(int s)&#123; for(int i=0;i&lt;V;i++) d[i] = INF; d[s] = 0; while(true) &#123; bool update = false; for(int i=0;i&lt;E;i++) &#123; edge e = es[i]; if(d[e.from]!=INF &amp;&amp; d[e.to]&gt;d[e.from]+e.cost) &#123; d[e.to] = d[e.from] + e.cost; update = true; &#125; &#125; if(!update) break; &#125; &#125; 如果图中不存在负圈，最短路径不会经过同一个顶点两次，while(true)最多执行|V|-1次（|V|个顶点，|V|-1条边），因此复杂度为 $O(|V|\times|E|)$。反之如果存在负圈，那么第|V|次循环种也会更新d的值，因此可以通过判断第V次是否仍更新了判断是否存在负圈。 Dijkstra算法在没有负边的情况下，上述Bellman-Ford算法复杂度高很大一部分原因是，如果d[i]还不是最短距离即使进行d[j]=d[i]+(从i到j的边的权值)的更新，d[j]也不会是最短距离，而且即使d[i]没有变化，每一次循坏也要检查一遍从i出发的所有边，很浪费时间。因此可以对算法做出如下修改： 找到最短距离已经确定的顶点，从它出发更新相邻顶点的最短距离 此后不用再关心1中的“最短距离已经确定的顶点” 下面是使用STL的priority_queue的实现。在每次更新时往堆里插入当前最短距离和顶点的值对，当取出的最小值不是最短距离时，就丢弃这个值。123456789101112131415161718192021222324252627struct edge&#123;int to, cost;&#125;;typedef pair&lt;int, int&gt; P;int V;vector&lt;edge&gt; G[MAX_V];int d[MAX_V];void dijkstra(int s)&#123; priority_queue&lt;P, vector&lt;P&gt;, greater&lt;P&gt; &gt; que; fill(d,d+V,INF); d[s] = 0; que.push(P(0,s)); while(!que.empty()) &#123; P p = que.top(); que.pop(); int v = p.second; if(d[v] &lt; p.first) continue; for(int i=0;i&lt;G[v].size();i++) &#123; edge e = G[v][i]; if(d[e.to] &gt; d[v]+e.cost) &#123; d[e.to] = d[v]+e.cost; que.push(P(d[e.to],e.to)); &#125; &#125; &#125;&#125; 2.5.6 图算法的应用 一共有n头牛，按编号顺序排成一排，有ml个关系好的牛的信息，有md个关系不好的牛的信息，对应输入的第一行的三个元素，接下来ml行，每行三个元素A,B,D，表示A牛和B牛相距不希望超过D，接下来md行，每行三个元素A,B,D表示A牛和B牛的相距至少要有D才行。求1号牛和n号牛的最大距离，如果距离无限大输出-2，如果无解输出-1。 记第i头牛的位置是d[i]。首先，牛是按照编号顺序排列的，所以有$d[i] \le d[i+1]$。]]></content>
      <categories>
        <category>三更有梦书为枕</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>图算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[加工并存储数据的数据结构——《挑战程序设计竞赛》]]></title>
    <url>%2F2019%2F06%2F25%2F%E5%8A%A0%E5%B7%A5%E5%B9%B6%E5%AD%98%E5%82%A8%E6%95%B0%E6%8D%AE%E7%9A%84%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E2%80%94%E2%80%94%E3%80%8A%E6%8C%91%E6%88%98%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%AB%9E%E8%B5%9B%E3%80%8B%2F</url>
    <content type="text"><![CDATA[2.4.4 并查集并查集的实现（交大《数据结构》教材中的实现）： 123456789101112131415161718192021int disjoint[500000]; //使用时别忘了先全部初始化为-1 int find(int x)&#123; if(disjoint[x] &lt; 0) return x; return disjoint[x] = find(disjoint[x]); //路径压缩&#125;void Union(int root1, int root2)&#123; if(root1 == root2) return ; if(disjoint[root1] &gt; disjoint[root2])// disjoint[root2]为负值，其绝对值为并查集的大小 &#123; disjoint[root2] += disjoint[root1]; disjoint[root1] =root2; &#125; else &#123; disjoint[root1] += disjoint[root2]; disjoint[root2] =root1; &#125;&#125; 并查集中根节点所存储的数据都为负值，其绝对值为当前根节点这一组中节点的个数。对于n个元素的并查集进行一次操作的复杂度（均摊复杂度）是$O(\alpha(n))$。在这里，$\alpha(n)$是阿克曼(Ackermann)函数的反函数，比$O(\log(n))$还要快。 动物王国中有三类动物A,B,C，这三类动物的食物链构成了有趣的环形。A吃B， B吃C，C吃A。 现有N个动物，以1－N编号。每个动物都是A,B,C中的一种，但是我们并不知道它到底是哪一种。有人用两种说法对这N个动物所构成的食物链关系进行描述：“1 X Y”，表示X和Y是同类。“2 X Y”，表示X吃Y。此人对N个动物，用上述两种说法，一句接一句地说出K句话，这K句话有的是真的，有的是假的。当一句话满足下列三条之一时，这句话就是假话，否则就是真话。1） 当前的话与前面的某些真的话冲突，就是假话；2） 当前的话中X或Y比N大，就是假话；3） 当前的话表示X吃X，就是假话。你的任务是根据给定的N（1 &lt;= N &lt;= 50,000）和K句话（0 &lt;= K &lt;= 100,000），输出假话的总数。 起初并不知道每只动物的种类，我想直到所有信息输入完全也很难推断出每只动物是啥种类，所以不妨对每只i创建3个元素i-A, i-B, i-C, 并用这$3\times N$个元素建立并查集，这个并查集维护如下信息： i-x表示“i属于种类x”。 并查集里的每一个组表示组内所有元素代表的情况都同时发生或不发生。 对于每一条信息，只需按照下面进行操作： 第一种，x和y属于同一种类：合并x-A和y-A、x-B和y-B、x-C和y-C。 第二种，x吃y： 合并x-A和y-B、x-B和y-C、x-C和y-A。 在合并之前判断这次合并是否会产生矛盾。例如在第一种信息的情况下，需要检查x-A和y-B或者y-C是否在同一组。代码如下123456789101112131415161718192021222324252627282930313233343536373839//省略了并查集的实现代码int N, K; //N种动物，K条信息int T[MAX_K], X[MAX_K], Y[MAX_K]; //T表示信息的类型int solve()&#123; int ans = 0; for(int i=0;i&lt;K;i++) &#123; int t = T[i]; int x = X[i] - 1, y = Y[i] - 1; if(x&lt;0 || N&lt;=x || y&lt;0 || N&lt;= y) &#123; ans++； continue; &#125; if(t==1) &#123; //x-A和y-B或者y-C是否在同一组，说明已经相信了全面一组信息说，x与y不同类 if(find(x) == find(y+N) || find(x) == find(y+2*N) ans++; else &#123; Union(x,y); Union(x+N,y+N); Union(x+2*N,y+2*N); &#125; &#125; else &#123; if(find(x) == find(y) || find(x) == find(y+2*N) ans++; else &#123; Union(x,y+N); Union(x+N,y+2*N); Union(x+2*N,y); &#125; &#125; &#125; return ans;&#125;]]></content>
      <categories>
        <category>三更有梦书为枕</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>并查集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录结果再利用的动态规划——《挑战程序设计竞赛》]]></title>
    <url>%2F2019%2F06%2F24%2F%E8%AE%B0%E5%BD%95%E7%BB%93%E6%9E%9C%E5%86%8D%E5%88%A9%E7%94%A8%E7%9A%84%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E2%80%94%E2%80%94%E3%80%8A%E6%8C%91%E6%88%98%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%AB%9E%E8%B5%9B%E3%80%8B%2F</url>
    <content type="text"><![CDATA[2.3.1 记忆化搜索与动态规划 01 背包问题有n个重量和价值分别为$w_i, v_i$的物品。从这些物品中挑选出总重量不超过W的物品，求所有挑选方案中价值总和的最大值。 循序渐进，先用最朴素的递归方法，针对每个物品是否放入背包进行搜索试试看。123456789101112131415161718192021222324252627int n, W;const int MAX_N = 10010;int w[MAX_N], v[MAX_N];//从index为i的物品开始挑选总重小于j的部分int rec(int i,int j)&#123; int res; if(i==n) //没有剩余的物品了 &#123; res = 0; &#125; else if(w[i]&gt;j)//index为i的物品重量大于剩余总重 &#123; res = rec(i+1,j); &#125; else &#123; res = max(rec(i+1,j),rec(i+1,j-w[i]) + v[i]); &#125; return res;&#125;void solve()&#123; printf("%d\n", rec(0,W));&#125; 这种方法搜索深度为n，如第19行所示，每一层都有两个分支，那么最坏的复杂度为$O(2^n)$。因为会有相同参数的rec的多次调用，重复计算，耗时费神。故记忆化搜索的想法是把第一次计算的结果记录下来，之后直接调用以防重复计算。1234567891011121314151617181920212223242526272829int dp[MAX_N+1][MAX_N+1];int rec(int i,int j)&#123; if(dp[i][j]&gt;=0) &#123; return dp[i][j]; &#125; int res; if(i==n) //没有剩余的物品了 &#123; res = 0; &#125; else if(w[i]&gt;j)//index为i的物品重量大于剩余总重 &#123; res = rec(i+1,j); &#125; else &#123; res = max(rec(i+1,j),rec(i+1,j-w[i]) + v[i]); &#125; dp[i][j] = res; return res;&#125;void solve()&#123; memset(dp,-1,sizeof(dp));//-1表示尚未被计算过，初始化整个数组 printf("%d\n", rec(0,W));&#125; 代码中直观地实现了我们记录第一次计算结果的想法，这就是记忆化搜索，也就是dp的萌芽。rec函数参数的组合最大nW种，所以时间复杂度为O(nW)。 memset进行初始化是按照1字节为单位对内存进行填充的，因为-1的二进制表示每一个比特位都是1，所以可以用memset初始化为-1，0也可以用memset初始化，但是不能初始化从1之类的数值，因为1的二进制表示为00000001，memset不能细粒度到每个比特。 但这并不是正经的DP格式，DP需给出递推关系，如下： 定义dp[i+1][j]:=从0到i这i+1个物品中选出总重量不超过j的物品时总价值的最大值,则有 \begin{equation} dp[0][j]=0 \\ dp[i+1][j] = \left\{ \begin{array}{**lr**} dp[i][j], & (j]]></content>
      <categories>
        <category>三更有梦书为枕</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>DP</tag>
        <tag>背包问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[最基础的穷竭搜索——《挑战程序设计竞赛》]]></title>
    <url>%2F2019%2F06%2F23%2F%E6%9C%80%E5%9F%BA%E7%A1%80%E7%9A%84%E7%A9%B7%E7%AB%AD%E6%90%9C%E7%B4%A2%E2%80%94%E2%80%94%E3%80%8A%E6%8C%91%E6%88%98%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%AB%9E%E8%B5%9B%E3%80%8B%2F</url>
    <content type="text"><![CDATA[2.1.4 深度优先搜索 [http://poj.org/problem?id=2386]: “Lake Counting” 有一个大小为N*M的园子，雨后积了很多水。八连通的积水被认为是在一起的。请求出园子里共有多少个水洼？(八连通是指下图中相对.的8 部分)wwww.wwww 从任意的’W’开始，不停地把邻接的部分用’.’代替，一次DFS(深度优先遍历)遍历后，与初始的这个 W 所连接的所有 ‘W’ 都会被替换成 ‘.’，因此直到图中没有 ‘W’为止，总共进行 DFS 的次数即为积水的次数。使用深度优先搜索，从任意W开始，进入DFS，在DFS中把八联通的邻接部分都’.’代替，若八连通区域中又有一个”W”，进入下一层DFS，直到当前的连通分支不再有W，总共DFS的次数就是答案。八连通对应着8个方向的状态转移，每个格子至多调用一次DFS，所以复杂度是$O(8\times N \times M) = O(N \times M)$。 2.1.6 特殊状态的枚举C++的algorithm库中提供了next_permutation这一函数，可以把n给元素共n!种不同的排列生成出来。 12345678910111213141516171819202122232425262728293031#include&lt;iostream&gt;using namespace std;const int MAX_N = 100;bool used[MAX_N];int perm[MAX_N];//生成&#123;0, 1, 2, 3,..., n-1&#125;的n!种排列 void permutation1(int pos, int n)&#123; if(pos == n)&#123; for(int i=0;i&lt;n;i++) cout&lt;&lt;perm[i]&lt;&lt;" "; cout&lt;&lt;endl; return; &#125; //针对perm的index为pos的位置，究竟使用0~n-1种的哪一个进行循环 for(int i = 0; i&lt; n; i++)&#123; if(!used[i])&#123; perm[pos] = i; used[i] = true; permutation1(pos + 1, n); used[i] = false; &#125; &#125; return;&#125;int main()&#123; permutation1(0,5); return 0;&#125; 调用函数时，主调函数所拥有的局部变量等信息需要存储在特定的内存区域，这个区域叫做栈内存区；另一方面，利用new或者malloc进行分配的内存区域叫做堆内存。栈内存在程序启动时被统一分配，此后不能再扩大，由于这一区域有上限，所以函数的递归深度也有上限。显式初始化全局变量被保存在数据段种，未显式初始化的全局变量保持在BSS段中。使用全局变量可以减小栈溢出的危险。 2.2 贪心法 [http://poj.org/problem?id=3617]: “Best Cow Line” 已知一段长度为N的字符串S，构造一个字典序最小的字符串T。起初T为空串，随后反复进行下列任意操作。-从S的头部删除一个字符，加到T的尾部-从S的尾部删除一个字符，加到T的尾部 贪心算法很容易想到：不断取S的开头和末尾中较小的一个字符放到T的末尾。如果两个字符相等，那么就不断比较下一个内部字符的大小。 [http://poj.org/problem?id=3253]: “Fence Repair” 有一位农夫要把一个木板(长度为 N 块木板长度之和)使用 (N-1) 次锯成 N 块给定长度的小木板，每次锯都要收取一定费用，这个费用就是当前锯的这个木板的长度，给定各个要求的小木板的长度，及小木板的个数 N，求最小的费用。 切割的方法可以参见如下的二叉树：1234567 15 / \ 7 8 / \ / \3 4 5 3 / \ 1 2 一个叶子就相当于一个小木板，那么开销的合计即为： \sum 小木板的长度 \times 小木板节点的深度此时最佳的切割方式为：最短的板与次短的板的节点应当是兄弟节点 不妨将$L_i$按大小顺序排列，那么最短的板$L_1$与次短的板$L_2$应当为兄弟节点，因为切割是自由的不妨当作$L_1$和$L_2$是最后切开的，那么这次切割之前就有： (L_1+L_2), L_3, L_4, ... ,L_N这样的N-1块木板存在。之后递归第将这N-1块木板的问题求解，每次加上最短的板与次短的板的长度之和即得解。时间复杂度为$O(N^2)$。事实上最佳的复杂度为$O(NlogN)$，之后将会学到。 123456789101112131415161718192021222324252627282930313233343536373839#include&lt;iostream&gt;using namespace std;int N;int L[50010];int main()&#123; cin&gt;&gt;N; for(int i=0;i&lt;N;i++) &#123; cin&gt;&gt;L[i]; &#125; long long ans = 0; while(N&gt;1) &#123; int mii1 = 0, mii2 = 1; if(L[mii1]&gt;L[mii2])swap(mii1,mii2); for(int i=2;i&lt;N;i++) &#123; if(L[i]&lt;L[mii1]) &#123; mii2 = mii1; mii1 = i; &#125; else if(L[i]&lt;L[mii2]) &#123; mii2 = i; &#125; &#125; int t = L[mii1] + L[mii2]; ans += t; if(mii1==N-1)swap(mii1,mii2); L[mii1] = t; L[mii2] = L[N-1]; N--; &#125; cout&lt;&lt;ans&lt;&lt;endl; return 0;&#125;]]></content>
      <categories>
        <category>三更有梦书为枕</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>贪心</tag>
      </tags>
  </entry>
</search>
